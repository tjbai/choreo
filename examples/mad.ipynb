{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899adacb-3176-435a-ac1c-e0b8b6f3de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4145544-2d20-497d-ba56-2db0a4c53327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbai4/llama3/llama/util.py:60: UserWarning: 65536 does not lie within [1, 8192]\n",
      "  warnings.warn(f\"{max_seq_len} does not lie within [1, 8192]\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded in 16.69 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama import Workflow, Llama\n",
    "from llama.util import find_free_port, load_ckpt\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
    "\n",
    "workflow = Workflow.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=8*8192,\n",
    "    max_batch_size=1,\n",
    "    model_parallel_size=1,\n",
    "    max_nodes=100,\n",
    ")\n",
    "\n",
    "llama = Llama(workflow.model, workflow.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46df5ad8-7e13-4324-8e4b-4ea10b36f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/5000 [00:02<14:09,  5.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     solutions \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(solutions))\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43meval_solutions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAnswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msolutions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproblems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(outputs))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/tbai4/llama3/dumps/mad/choreo_ft_correct.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/llama3/llama/workflows/tot.py:1180\u001b[0m, in \u001b[0;36meval_solutions\u001b[0;34m(llama, solutions, problems)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m soln, prob \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(solutions, problems), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(problems)):\n\u001b[1;32m   1172\u001b[0m     dialog \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1174\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: evaluator_prompt\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROBLEM:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprob[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproblem\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGROUND TRUTH:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprob[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolution\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mATTEMPT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msoln\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1178\u001b[0m     }]\n\u001b[0;32m-> 1180\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdialog\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1188\u001b[0m     incorrect_votes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincorrect\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m o[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m   1189\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(incorrect_votes \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/llama3/llama/generation.py:265\u001b[0m, in \u001b[0;36mLlama.chat_completion\u001b[0;34m(self, dialogs, content_prefills, temperature, top_p, max_gen_len, log_probs, seed)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     content_prefills \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m dialogs]\n\u001b[0;32m--> 265\u001b[0m generation_tokens, generation_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_probs:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    275\u001b[0m         {\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m         )\n\u001b[1;32m    288\u001b[0m     ]\n",
      "File \u001b[0;32m~/llama3/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llama3/llama/generation.py:145\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, prompt_tokens, max_gen_len, temperature, top_p, log_probs, echo, seed)\u001b[0m\n\u001b[1;32m    141\u001b[0m     eos_reached \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m~\u001b[39minput_text_mask[:, cur_pos]) \u001b[38;5;241m&\u001b[39m (\n\u001b[1;32m    142\u001b[0m         torch\u001b[38;5;241m.\u001b[39misin(next_token, stop_tokens)\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     prev_pos \u001b[38;5;241m=\u001b[39m cur_pos\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meos_reached\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_probs:\n",
      "File \u001b[0;32m~/llama3/.venv/lib/python3.12/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from llama.workflows.tot import load_math_problems, eval_solutions\n",
    "\n",
    "llama.model.reshape_cache(4)\n",
    "\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='test')\n",
    "with open('/home/tbai4/llama3/dumps/mad/postft_eval_test.json') as f:\n",
    "    solutions = json.load(f)\n",
    "    print(len(solutions))\n",
    "    \n",
    "outputs = eval_solutions(\n",
    "    llama,\n",
    "    [s['outputs']['decision']['Answer'] for s in solutions if isinstance(s['outputs']['decision'], dict)],\n",
    "    problems,\n",
    ")\n",
    "\n",
    "with open('/home/tbai4/llama3/dumps/mad/choreo_ft_correct.json', 'w') as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3bf0e7-1467-4581-ab6c-7d06d84439d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:46<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from llama.workflows.tot import load_math_problems, eval_solutions\n",
    "\n",
    "with open('/home/tbai4/llama3/dumps/mad/postft_eval.json') as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))\n",
    "\n",
    "llama.model.reshape_cache(4)\n",
    "\n",
    "solutions = []\n",
    "for d in data:\n",
    "    outputs = d.get('outputs', {})\n",
    "    if 'decision' in outputs and outputs['decision'] is not None:\n",
    "        solutions.append(outputs['decision']['Answer'])\n",
    "    else:\n",
    "        solutions.append('')\n",
    "        \n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='val')\n",
    "\n",
    "outputs = eval_solutions(\n",
    "    llama,\n",
    "    solutions,\n",
    "    problems[:len(solutions)]\n",
    ")\n",
    "\n",
    "sum(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b980894-85c4-43fa-be06-03c2e4462748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:55<00:00,  4.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/tbai4/llama3/dumps/mad/math_baseline_e2e.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "solutions = []\n",
    "for d in data:\n",
    "    outputs = d.get('outputs', {})\n",
    "    if 'decision' in outputs and isinstance(outputs['decision'], dict):\n",
    "        solutions.append(outputs['decision']['Answer'])\n",
    "    else:\n",
    "        solutions.append('')\n",
    "    \n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='train')[:500]\n",
    "\n",
    "outputs = eval_solutions(\n",
    "    llama,\n",
    "    solutions,\n",
    "    problems[:len(solutions)]\n",
    ")\n",
    "asdfasdf\n",
    "sum(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949d650f-fbe4-44f0-a152-27e2dc183f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ckpt(workflow, '/scratch4/jeisner1/tjbai/checkpoints/mad/lora_step-899.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b8d0eb1-9890-4ff1-aef8-e55c35a8ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from llama.workflows.mad import mad_baseline, mad_cached\n",
    "from llama.workflows.tot import load_math_problems\n",
    "from llama.workflows.simple import math_direct\n",
    "\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='val')[:100]\n",
    "\n",
    "workflow.reset()\n",
    "outputs = mad_cached(\n",
    "    workflow=workflow,\n",
    "    problem=problems[0]['problem'],\n",
    "    max_rounds=3,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f6d3cf-485b-4b1b-b975-e732f237fd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\"Preference\": \"No\", \"Supported Side\": \"\", \"Reason\": \"Both sides agree on the non-deterministic behavior of the function and the final answer, but the negative side does not provide a clear preference for the affirmative side\\'s analysis. The debate will continue to the next round.\", \"Answer\": \"\"}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''\n",
    "{\"Preference\": \"No\", \"Supported Side\": \"\", \"Reason\": \"Both sides agree on the non-deterministic behavior of the function and the final answer, but the negative side does not provide a clear preference for the affirmative side's analysis. The debate will continue to the next round.\", \"Answer\": \"\"}\n",
    "'''\n",
    "\n",
    "from llama.workflows.mad import parse_decision\n",
    "\n",
    "parse_decision(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6211213-c02a-46e5-b48c-7d6331d3deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('tmp') as f:\n",
    "    sample = json.load(f)\n",
    "    print(len(sample['outputs']['mod_tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b762618-cbfe-4f2d-a892-512f657657f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 54.5M / 8.1B parameters\n"
     ]
    }
   ],
   "source": [
    "from llama.workflows.finetune import MadTrainer\n",
    "\n",
    "trainer = MadTrainer(\n",
    "    workflow=workflow,\n",
    "    output_dir='/scratch4/jeisner1/tjbai/checkpoints/bsm/',\n",
    "    learning_rate=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacda09-2e90-48fb-a8be-9078485997b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    loss, metrics = trainer.step(sample, debug=False)\n",
    "    loss.backward()\n",
    "    trainer.optimizer.step()\n",
    "    trainer.optimizer.zero_grad()    \n",
    "    if (i+1) % 5 == 0:\n",
    "        print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f010a03-530a-41f7-b849-f1e8004a8cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected chunk: initial (weights: {'initial': 2, 'rounds_first_half': 3, 'rounds_second_half': 3, 'final': 1})\n",
      "Total loss value: 4.3199\n",
      "Selected chunk: initial (loss: 0.4768067002296448)\n"
     ]
    }
   ],
   "source": [
    "loss = trainer.step(sample, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d03395-efe7-4e7a-9640-0d25e6b2cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss[0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48262aae-c653-4b1a-aafe-df62f6ffb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from llama.workflows.mad_iterative import math_simple_baseline\n",
    "from llama.workflows.tot import load_math_problems\n",
    "\n",
    "# MATH dataset\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='val')\n",
    "\n",
    "# baseline with reflection on MATH\n",
    "solutions = []\n",
    "for problem in tqdm(problems):\n",
    "    workflow.reset()\n",
    "    solutions.append(math_simple_baseline(\n",
    "        workflow=workflow,\n",
    "        problem=problem['problem'],\n",
    "        enable_reflection=True,\n",
    "        debug=False,\n",
    "    ))\n",
    "    \n",
    "with open('math_baseline_with_reflection.json', 'w') as f:\n",
    "    json.dump(solutions, f)\n",
    "\n",
    "# baseline without reflection on MATH\n",
    "solutions = []\n",
    "for problem in tqdm(problems):\n",
    "    workflow.reset()\n",
    "    solutions.append(math_simple_baseline(\n",
    "        workflow=workflow,\n",
    "        problem=problem['problem'],\n",
    "        enable_reflection=False,\n",
    "        debug=False,\n",
    "    ))\n",
    "\n",
    "with open('math_baseline_without_reflection.json', 'w') as f:\n",
    "    json.dump(solutions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf38fd-eb90-41f8-adef-882e6c47781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from llama.workflows.mad_iterative import math_mad_cached, math_simple_baseline, load_ciar\n",
    "from llama.workflows.tot import load_math_problems\n",
    "\n",
    "# CIAR dataset\n",
    "problems = load_ciar('/home/tbai4/llama3/data/CIAR', start=0, end=50)\n",
    "\n",
    "# MAD cached on CIAR\n",
    "solutions = []\n",
    "for problem in tqdm(problems):\n",
    "    workflow.reset()\n",
    "    solutions.append(math_mad_cached(\n",
    "        workflow=workflow,\n",
    "        problem=problem['question'],\n",
    "        max_rounds=3,\n",
    "    ))\n",
    "with open('improved_ciar_cached.json', 'w') as f:\n",
    "    json.dump(solutions, f)\n",
    "\n",
    "# baseline with reflection on CIAR\n",
    "solutions = []\n",
    "for problem in tqdm(problems):\n",
    "    workflow.reset()\n",
    "    solutions.append(math_simple_baseline(\n",
    "        workflow=workflow,\n",
    "        problem=problem['question'],\n",
    "        enable_reflection=True,\n",
    "    ))\n",
    "with open('ciar_baseline_with_reflection.json', 'w') as f:\n",
    "    json.dump(solutions, f)\n",
    "\n",
    "# baseline without reflection on CIAR\n",
    "solutions = []\n",
    "for problem in tqdm(problems):\n",
    "    workflow.reset()\n",
    "    solutions.append(math_simple_baseline(\n",
    "        workflow=workflow,\n",
    "        problem=problem['question'],\n",
    "        enable_reflection=False,\n",
    "    ))\n",
    "with open('ciar_baseline_without_reflection.json', 'w') as f:\n",
    "    json.dump(solutions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f99b52-0473-4aa0-ad02-49bd27e7429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from llama.workflows.mad_iterative import mad_baseline, mad_cached, simple_baseline, load_translations\n",
    "\n",
    "translations = load_translations('/home/tbai4/llama3/data/commonmt', start=0, end=100)\n",
    "\n",
    "for translation in tqdm(translations):\n",
    "    workflow.reset()\n",
    "    simple_baseline(workflow, translation['chinese'], debug=False, enable_reflection=True)\n",
    "    \n",
    "for translation in tqdm(translations):\n",
    "    workflow.reset()\n",
    "    simple_baseline(workflow, translation['chinese'], debug=False, enable_reflection=False)\n",
    "    \n",
    "for translation in tqdm(translations):\n",
    "    workflow.reset()\n",
    "    mad_baseline(workflow, translation['chinese'], agents=['Alice', 'Bob'], max_rounds=3, debug=False)\n",
    "    \n",
    "for translation in tqdm(translations):\n",
    "    workflow.reset()\n",
    "    simple_baseline(workflow, translation['chinese'], agents=['Alice', 'Bob'], max_rounds=3, debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
