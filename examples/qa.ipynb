{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8d4292-6031-45b3-a44c-5e0988dae8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e22b1fc-910a-41b0-9d4a-3db0d1b606e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Converting to LoRA\n",
      "Loaded in 16.92 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama import Workflow, Llama\n",
    "from llama.util import find_free_port\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
    "\n",
    "workflow = Workflow.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=8192,\n",
    "    max_batch_size=8,\n",
    "    model_parallel_size=1,\n",
    "    max_nodes=100,\n",
    "    use_lora=True,\n",
    "    lora_rank=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "llama = Llama(workflow.model, workflow.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc5b083-f642-46e7-a313-0de122cb94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('/home/tbai4/llama3/data/triviaqa/unfiltered-web-train.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     problems = data['Data']\n",
    "#     problems = problems[:len(problems)]\n",
    "\n",
    "# with open('/home/tbai4/llama3/data/triviaqa/unfiltered-web-dev.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     problems = data['Data']\n",
    "#     problems = problems[:len(problems)//2] # hold out half"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c996dfb8-6e5e-46d0-83d4-e5dd97a9efe0",
   "metadata": {},
   "source": [
    "## accuracy prior to fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1569b56-4ddf-460d-bf61-eb97d7859b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from llama.workflows.qa import ask_parallel, eval_system_prompt, format_eval_user, parse_items\n",
    "\n",
    "workflow.model.eval()\n",
    "\n",
    "with open('/home/tbai4/llama3/data/triviaqa/unfiltered-web-dev.json') as f:\n",
    "    data = json.load(f)\n",
    "    problems = data['Data']\n",
    "    \n",
    "for num_questions in [16]:\n",
    "    answers = []\n",
    "    for seed in tqdm(range(50), desc='Generating'):\n",
    "        workflow.reset()\n",
    "        random.seed(seed)\n",
    "        subset = random.sample(problems, k=num_questions)\n",
    "        answer = ask_parallel(workflow, subset, annotate=True)\n",
    "        answers.append((subset, workflow.tokenizer.decode(answer['output_tokens'])))\n",
    "\n",
    "    workflow.model.reshape_cache(num_questions)\n",
    "    workflow.model.set_adapter_state(enabled=False)\n",
    "    correct = defaultdict(int)\n",
    "    for subset, answer in tqdm(answers, desc='Evaluating'):\n",
    "        individual_answers = parse_items(answer)\n",
    "        resps = llama.chat_completion([\n",
    "            [{'role': 'system', 'content': eval_system_prompt},\n",
    "            {'role': 'user', 'content': format_eval_user(s, a)}]\n",
    "            for s, a in zip(subset, individual_answers)\n",
    "        ], content_prefills=['{\"correct\": \"'] * min(num_questions, len(individual_answers)))\n",
    "\n",
    "        for i, r in enumerate(resps):\n",
    "            if 'true' in r['generation']['content'].lower():\n",
    "                correct[i] += 1\n",
    "\n",
    "    print(sorted(list(correct.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f799241-7ff4-4cfd-9ea1-fcc6d9cefe8c",
   "metadata": {},
   "source": [
    "## N=2, sequential (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cea0e884-4cc3-4dba-bdf6-b3bb39b55902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:44<00:00,  1.50s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13163"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from llama.workflows.qa import ask_sequential\n",
    "\n",
    "workflow.model.set_adapter_state(enabled=False)\n",
    "\n",
    "output = []\n",
    "for seed in tqdm(range(30)):\n",
    "    random.seed(seed)\n",
    "    output.append(f\"\\n## Trial {seed}\\n\")\n",
    "    subset = random.sample(problems, k=2)\n",
    "    answer = ask_sequential(workflow, subset)\n",
    "    answer = workflow.tokenizer.decode(answer['output_tokens'])\n",
    "\n",
    "    for i, (s, answer) in enumerate(zip(subset, parse_items(answer))):\n",
    "        output.extend([\n",
    "            f\"### Question {i+1}\\n\",\n",
    "            f\"**Question:** {s['Question']}\\n\", \n",
    "            f\"**Ground Truth:** {s['Answer']['Value']}\\n\",\n",
    "            f\"**Generated:** {answer}\\n\"\n",
    "        ])\n",
    "        if i < len(subset) - 1:\n",
    "            output.append(\"\\n---\\n\")\n",
    "\n",
    "Path('/home/tbai4/llama3/dumps/sequential_dev_n2.md').write_text('\\n'.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd779e-00da-46a1-a752-1dd03f0eebaa",
   "metadata": {},
   "source": [
    "## N=2, Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b767bfe-a467-4893-b842-2235de1971ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "annotate=False, compact=False: 100%|██████████| 30/30 [00:17<00:00,  1.74it/s]\n",
      "annotate=True, compact=False: 100%|██████████| 30/30 [00:17<00:00,  1.75it/s]\n",
      "annotate=False, compact=True: 100%|██████████| 30/30 [00:16<00:00,  1.81it/s]\n",
      "annotate=True, compact=True: 100%|██████████| 30/30 [00:18<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "configs = [\n",
    "   ('parallel_base_n2.md', False, False),\n",
    "   ('parallel_annotated_n2.md', True, False), \n",
    "   ('parallel_linearized_n2.md', False, True),\n",
    "   ('parallel_annotated_linearized_n2.md', True, True)\n",
    "]\n",
    "\n",
    "for filename, annotate, compact in configs:\n",
    "    output = []\n",
    "    for seed in tqdm(range(30), desc=f\"annotate={annotate}, compact={compact}\"):\n",
    "        random.seed(seed)\n",
    "        output.append(f\"\\n## Trial {seed}\\n\")\n",
    "        subset = random.sample(problems, k=2)\n",
    "        answer = ask_parallel(workflow, subset, annotate=annotate, compact=compact)\n",
    "\n",
    "        for i, (s, answer) in enumerate(zip(subset, parse_items(answer))):\n",
    "            output.extend([\n",
    "                f\"### Question {i+1}\\n\",\n",
    "                f\"**Question:** {s['Question']}\\n\", \n",
    "                f\"**Ground Truth:** {s['Answer']['Value']}\\n\",\n",
    "                f\"**Generated:** {answer}\\n\"\n",
    "            ])\n",
    "            \n",
    "        if i < len(subset) - 1:\n",
    "            output.append(\"\\n---\\n\")\n",
    "\n",
    "    Path(f'/home/tbai4/llama3/dumps/triviaqa/{filename}').write_text('\\n'.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697b331-0abf-4773-9786-c06d14d406fd",
   "metadata": {},
   "source": [
    "## exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32a72e2f-ea9c-40a0-ba36-b9ef44a40a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Chicago\n",
      "2. Richard Nixon\n",
      "3. Portugal\n",
      "4. Chicago Bears\n"
     ]
    }
   ],
   "source": [
    "res = llama.chat_completion(\n",
    "    dialogs=[[\n",
    "        {'role': 'system', 'content': 'Answer ALL of the user\\'s questions. Answer with an numbered list. Do not include extraneous text.'},\n",
    "        {'role': 'user', 'content': 'Which city does David Soul come from?'},\n",
    "        {'role': 'user', 'content': 'Who was President when the first Peanuts cartoon was published?'},\n",
    "        {'role': 'user', 'content': 'From which country did Angola achieve independence in 1975?'},\n",
    "        {'role': 'user', 'content': 'Who won Super Bowl XX?'},\n",
    "    ]]\n",
    ")\n",
    "\n",
    "print(res[0]['generation']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68f66bd6-c4c5-4c51-a871-0a4281ef26c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Chicago\n",
      "2. Richard Nixon\n",
      "3. Portugal\n",
      "4. Chicago Bears\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter as get\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'Answer ALL of the user\\'s questions. Answer with an numbered list. Do not include extraneous text.'},\n",
    "    {'role': 'user', 'content': 'Which city does David Soul come from?'},\n",
    "    {'role': 'user', 'content': 'Who was President when the first Peanuts cartoon was published?'},\n",
    "    {'role': 'user', 'content': 'From which country did Angola achieve independence in 1975?'},\n",
    "    {'role': 'user', 'content': 'Who won Super Bowl XX?'}\n",
    "]\n",
    "\n",
    "[prompt] = workflow.insert([{'messages': messages, 'parent_ids': []}])\n",
    "\n",
    "[response] = get('tokens')(workflow.step(\n",
    "    [{\n",
    "        'header': ('assistant', None),\n",
    "        'prefill': '',\n",
    "        'parent_ids': [prompt['id']]\n",
    "    }]\n",
    "))\n",
    "\n",
    "print(workflow.tokenizer.decode(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ec22d1b-76cd-4909-aa21-2adcaa030b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Gerald Ford\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter as get\n",
    "\n",
    "workflow.reset()\n",
    "\n",
    "[prompt] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer ALL of the user\\'s questions. Answer with an numbered list. Do not include extraneous text.'}],\n",
    "        'parent_ids': []\n",
    "    }\n",
    "])\n",
    "\n",
    "questions = workflow.insert([\n",
    "        {\n",
    "            'messages': [{'role': 'user', 'content': 'Question 1: Which city does David Soul come from?'}],\n",
    "            'parent_ids': [prompt['id']],\n",
    "        },\n",
    "        {\n",
    "            'messages': [{'role': 'user', 'content': 'Question 4: From which country did Angola achieve independence in 1975?'},],\n",
    "            'parent_ids': [prompt['id']],\n",
    "        },\n",
    "        {\n",
    "            'messages': [{'role': 'user', 'content': 'Question 3: Who won Super Bowl XX?'},],\n",
    "            'parent_ids': [prompt['id']],\n",
    "        },\n",
    "        {\n",
    "            'messages': [{'role': 'user', 'content': 'Question 2: Who was President when the first Peanuts cartoon was published?'},],\n",
    "            'parent_ids': [prompt['id']],\n",
    "        },\n",
    "])\n",
    "\n",
    "[response] = get('tokens')(workflow.step(\n",
    "    tasks=[{\n",
    "        'header': ('assistant', None),\n",
    "        'prefill': '',\n",
    "        'parent_ids': [prompt['id']] + [q['id'] for q in questions],\n",
    "    }],\n",
    "    compact=True\n",
    "))\n",
    "\n",
    "print(workflow.tokenizer.decode(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9bb9f883-5e87-45b6-a48d-9e9da8122afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Chicago\n",
      "2. United States\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter as get\n",
    "\n",
    "workflow.reset()\n",
    "\n",
    "[prompt] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer ALL of the user\\'s questions. Answer with an numbered list. Do not include extraneous text.'}],\n",
    "        'parent_ids': []\n",
    "    }\n",
    "])\n",
    "\n",
    "questions = workflow.insert([\n",
    "        {\n",
    "            'messages': [\n",
    "                {'role': 'user', 'content': 'Which city does David Soul come from?'},\n",
    "                {'role': 'user', 'content': 'From which country did Angola achieve independence in 1975?'},\n",
    "            ],\n",
    "            'parent_ids': [prompt['id']],\n",
    "        },\n",
    "        {\n",
    "            'messages': [\n",
    "                {'role': 'user', 'content': 'Who won Super Bowl XX?'},\n",
    "                {'role': 'user', 'content': 'Who was President when the first Peanuts cartoon was published?'},\n",
    "            ],\n",
    "            'parent_ids': [prompt['id']],\n",
    "        },\n",
    "])\n",
    "\n",
    "[response] = get('tokens')(workflow.step(\n",
    "    tasks=[{\n",
    "        'header': ('assistant', None),\n",
    "        'prefill': '',\n",
    "        'parent_ids': [prompt['id']] + [q['id'] for q in questions],\n",
    "    }],\n",
    "))\n",
    "\n",
    "print(workflow.tokenizer.decode(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
