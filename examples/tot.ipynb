{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ca7760-6bfc-438b-b65a-5fb4f4ba9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dfb45fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbai4/llama3/llama/util.py:60: UserWarning: 16384 does not lie within [1, 8192]\n",
      "  warnings.warn(f\"{max_seq_len} does not lie within [1, 8192]\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded in 45.66 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama import Workflow, Llama\n",
    "from llama.util import find_free_port\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
    "\n",
    "workflow = Workflow.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=2*8192,\n",
    "    max_batch_size=5,\n",
    "    model_parallel_size=1,\n",
    "    max_nodes=100,\n",
    ")\n",
    "\n",
    "llama = Llama(workflow.model, workflow.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc16d4b-044a-475c-a9bc-bdbf5e8eb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter as get\n",
    "\n",
    "from llama.workflows.tot import (\n",
    "    load_math_problems,\n",
    "    cot_prompt,\n",
    "    format_problem,\n",
    "    format_vote_system_prompt,\n",
    "    finish_prompt\n",
    ")\n",
    "\n",
    "def baseline(\n",
    "    workflow,\n",
    "    problem,\n",
    "    branching_factor=8,\n",
    "    voters=4,\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "):\n",
    "    workflow.reset()\n",
    "    insert_time = []\n",
    "    step_time = []\n",
    "    ttft_time = []\n",
    "    total_ttft = 0\n",
    "    total_tokens = 0\n",
    "    force_tokens = []\n",
    "    s = time.time()\n",
    "\n",
    "    [cot] = workflow.insert([\n",
    "        {'messages': [\n",
    "            {'role': 'system', 'content': cot_prompt},\n",
    "            {'role': 'user', 'content': format_problem(problem)}\n",
    "        ], 'parent_ids': []},\n",
    "    ], time_buffer=insert_time)\n",
    "    proposal_tokens, proposal_nodes = get('tokens', 'nodes')(workflow.step([\n",
    "            {'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [cot['id']]}\n",
    "            for i in range(branching_factor)\n",
    "        ],\n",
    "        compact=False,\n",
    "        max_gen_len=512,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        seed=42,\n",
    "        time_buffer=step_time,\n",
    "        ttft_buffer=ttft_time,\n",
    "    ))\n",
    "    total_ttft += insert_time[-1] + ttft_time[-1]\n",
    "    total_tokens += sum(len(a) for a in proposal_tokens)\n",
    "    force_tokens.append(max(len(p) for p in proposal_tokens))\n",
    "\n",
    "    vote_user_prompt = f'{format_problem(problem)}\\n\\nHere are the proposals:'\n",
    "    for i, prop in enumerate(proposal_tokens):\n",
    "        vote_user_prompt += f'\\n\\nSolution #{i+1}:\\n{workflow.tokenizer.decode(prop)}'\n",
    "    [vote] = workflow.insert([\n",
    "        {'messages': [\n",
    "            {'role': 'system', 'content': format_vote_system_prompt(branching_factor)},\n",
    "            {'role': 'system', 'content': vote_user_prompt}\n",
    "        ], 'parent_ids': []}\n",
    "    ], time_buffer=insert_time)\n",
    "    vote_tokens = get('tokens')(workflow.step([\n",
    "            {'header': ('assistant', None),\n",
    "            'prefill': 'BEST CHOICE: ',\n",
    "            'parent_ids': [vote['id']]}\n",
    "            for _ in range(voters)\n",
    "        ],\n",
    "        compact=False,\n",
    "        max_gen_len=256,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        seed=42,\n",
    "        time_buffer=step_time,\n",
    "        ttft_buffer=ttft_time\n",
    "    ))\n",
    "    total_ttft += insert_time[-1] + ttft_time[-1]\n",
    "    total_tokens += sum(len(a) for a in vote_tokens)\n",
    "    force_tokens.append(max(len(p) for p in vote_tokens))\n",
    "\n",
    "    # doesn't matter which is best, we should just simulate always\n",
    "    best_proposal = workflow.tokenizer.decode(proposal_tokens[0])\n",
    "    [finish] = workflow.insert([\n",
    "        {'messages': [\n",
    "            {\"role\": \"system\", \"content\": finish_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"{format_problem(problem)}\\n\\nHere is the proposed approach: {best_proposal}\"}],\n",
    "        'parent_ids': []\n",
    "    }], time_buffer=insert_time)\n",
    "    [final_tokens] = get('tokens')(workflow.step([\n",
    "            {'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [finish['id']]}\n",
    "        ],\n",
    "        max_gen_len=256,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        time_buffer=step_time,\n",
    "        ttft_buffer=ttft_time\n",
    "    ))\n",
    "    total_ttft += insert_time[-1] + ttft_time[-1]\n",
    "    total_tokens += len(final_tokens)\n",
    "    force_tokens.append(len(final_tokens))\n",
    "\n",
    "    return {\n",
    "        'wall_time': time.time() - s,\n",
    "        'cuda_time': sum(insert_time) + sum(step_time),\n",
    "        'ttft': total_ttft,\n",
    "        'tokens': total_tokens,\n",
    "        'force_tokens': force_tokens\n",
    "    }\n",
    "\n",
    "def cached(\n",
    "    workflow,\n",
    "    problem,\n",
    "    branching_factor=8,\n",
    "    voters=4,\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "    force_tokens=[],\n",
    "):\n",
    "    workflow.reset()\n",
    "    assert len(force_tokens) == 3\n",
    "\n",
    "    insert_time = []\n",
    "    step_time = []\n",
    "    ttft_time = []\n",
    "    total_ttft = 0\n",
    "    s = time.time()\n",
    "\n",
    "    cot, vote, finish = workflow.insert([\n",
    "        {'messages': [\n",
    "            {'role': 'system', 'content': cot_prompt},\n",
    "            {'role': 'user', 'content': format_problem(problem)}\n",
    "        ], 'parent_ids': []},\n",
    "        {'messages': [\n",
    "            {'role': 'system', 'content': format_vote_system_prompt(branching_factor)},\n",
    "            {'role': 'user', 'content': format_problem(problem)}\n",
    "        ], 'parent_ids': []},\n",
    "        {'messages': [\n",
    "            {'role': 'system', 'content': finish_prompt},\n",
    "            {'role': 'user', 'content': format_problem(problem)}\n",
    "        ], 'parent_ids': []},\n",
    "    ], time_buffer=insert_time)\n",
    "\n",
    "    proposal_tokens, proposal_nodes = get('tokens', 'nodes')(workflow.step([\n",
    "            {'header': ('assistant', None),\n",
    "            'prefill': f'Solution #{i+1}:\\n\\n',\n",
    "            'parent_ids': [cot['id']]}\n",
    "            for i in range(branching_factor)\n",
    "        ],\n",
    "        compact=False,\n",
    "        max_gen_len=512,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        force_tokens=force_tokens.pop(0),\n",
    "        time_buffer=step_time,\n",
    "        ttft_buffer=ttft_time,\n",
    "    ))\n",
    "    total_ttft += insert_time[-1] + ttft_time[-1]\n",
    "\n",
    "    vote_tokens, vote_nodes = get('tokens', 'nodes')(workflow.step([\n",
    "            {'header': ('assistant', None),\n",
    "            'prefill': 'BEST CHOICE: ',\n",
    "            'parent_ids': [vote['id']] + [p['id'] for p in proposal_nodes]}\n",
    "            for _ in range(voters)\n",
    "        ],\n",
    "        stateless=False,\n",
    "        compact=False,\n",
    "        max_gen_len=256,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        force_tokens=force_tokens.pop(0),\n",
    "        time_buffer=step_time,\n",
    "        ttft_buffer=ttft_time,\n",
    "    ))\n",
    "    total_ttft += ttft_time[-1]\n",
    "\n",
    "    [final_tokens] = get('tokens')(workflow.step([\n",
    "            {'header': ('assistant', None),\n",
    "            'prefill': 'ANSWER: ',\n",
    "            'parent_ids': [finish['id']] + [proposal_nodes[0]['id']]}\n",
    "        ],\n",
    "        stateless=False,\n",
    "        max_gen_len=256,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        time_buffer=step_time,\n",
    "        ttft_buffer=ttft_time,\n",
    "        force_tokens=force_tokens.pop(0)\n",
    "    ))\n",
    "    total_ttft += ttft_time[-1]\n",
    "\n",
    "    return {\n",
    "        'wall_time': time.time() - s,\n",
    "        'cuda_time': sum(insert_time) + sum(step_time),\n",
    "        'ttft': total_ttft,\n",
    "    }\n",
    "\n",
    "def benchmark(*args, **kwargs):\n",
    "    baseline_res = baseline(*args, **kwargs)\n",
    "    print(baseline_res)\n",
    "    cached_res = cached(*args, **kwargs, force_tokens=baseline_res['force_tokens'])\n",
    "    return baseline_res, cached_res\n",
    "\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='val')[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02515ae8-7db1-460e-ae59-eb9e7e65d787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 16.93652828979492, 'ttft': 0.38639542388916015, 'tokens': 1495, 'force_tokens': [217, 50, 137]}\n"
     ]
    }
   ],
   "source": [
    "baseline_res, cached_res = benchmark(workflow, problems[0]['problem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2419fa5f-bf47-4fee-9b6e-0884c454690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 16.93652828979492, 'ttft': 0.38639542388916015, 'tokens': 1495, 'force_tokens': []}\n",
      "{'time': 16.640416851043703, 'ttft': 0.18356931304931642}\n"
     ]
    }
   ],
   "source": [
    "print(baseline_res)\n",
    "print(cached_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245fbbb0-bfb9-499a-9ec7-d35e9284bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.990097045898438\n",
      "120\n",
      "3.6810128688812256\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "s = time.time()\n",
    "[final_tokens] = get('tokens')(workflow.step([\n",
    "        {'header': ('assistant', None),\n",
    "        'prefill': '',\n",
    "        'parent_ids': [finish['id']]}\n",
    "    ],\n",
    "    max_gen_len=1024,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    time_buffer=step_time,\n",
    "    ttft_buffer=ttft_time,\n",
    "    force_tokens=512,\n",
    "))\n",
    "print(time.time() - s)\n",
    "print(len(final_tokens))\n",
    "\n",
    "s = time.time()\n",
    "[final_tokens] = get('tokens')(workflow.step([\n",
    "        {'header': ('assistant', None),\n",
    "        'prefill': '',\n",
    "        'parent_ids': [finish['id']]}\n",
    "    ],\n",
    "    max_gen_len=1024,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    time_buffer=step_time,\n",
    "    ttft_buffer=ttft_time,\n",
    "))\n",
    "print(time.time() - s)\n",
    "print(len(final_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c551cd-5e08-4b42-b1ad-4b153df1d0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANS'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.tokenizer.decode(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e17051e-9bf6-455a-843e-9268f65846fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 14.470890384674071\n",
      "total ttft 0.37766575622558596\n",
      "tokens generate 1445\n"
     ]
    }
   ],
   "source": [
    "print('total time', sum(insert_time) + sum(step_time))\n",
    "print('total ttft', total_ttft)\n",
    "print('tokens generate', total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9f07ee-6a33-43aa-9f10-c77811290432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import statistics\n",
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter as get\n",
    "\n",
    "from llama import Workflow\n",
    "from llama.workflows.benchmark import measure_step\n",
    "from llama.workflows.tot import load_math_problems\n",
    "from llama.workflows.tot import (\n",
    "    cot_prompt,\n",
    "    finish_prompt,\n",
    "    format_problem,\n",
    "    format_vote_system_prompt,\n",
    "    parse_choice,\n",
    ")\n",
    "\n",
    "def benchmark(\n",
    "    workflow: Workflow,\n",
    "    problem: str,\n",
    "    branching_factor: int = 8,\n",
    "    voters: int = 4,\n",
    "    num_runs: int = 3,\n",
    "):\n",
    "    metrics = {\n",
    "        \"baseline\": {\"tps\": [], \"latency\": [], \"cuda_latency\": [], \"ttft\": [], \"steps\": []},\n",
    "        \"cached\": {\"tps\": [], \"latency\": [], \"cuda_latency\": [], \"ttft\": [], \"steps\": []}\n",
    "    }\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run+1}/{num_runs}\")\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # ====== BASELINE ======\n",
    "        workflow.reset()\n",
    "        baseline_start = time.time()\n",
    "        cuda_start_event = torch.cuda.Event(enable_timing=True)\n",
    "        cuda_end_event = torch.cuda.Event(enable_timing=True)\n",
    "        cuda_start_event.record(stream=None)\n",
    "\n",
    "        baseline_step_metrics = []\n",
    "        total_tokens = 0\n",
    "\n",
    "        # Start timing complete TTFT for proposal step (including insert)\n",
    "        proposal_ttft_start = time.time()\n",
    "        \n",
    "        [cot] = workflow.insert([\n",
    "            {'messages': [\n",
    "                {'role': 'system', 'content': cot_prompt},\n",
    "                {'role': 'user', 'content': format_problem(problem)}\n",
    "            ], 'parent_ids': []},\n",
    "        ])\n",
    "\n",
    "        proposal_step_args = {\n",
    "            'tasks': [\n",
    "                {'header': ('assistant', None),\n",
    "                'prefill': '',\n",
    "                'parent_ids': [cot['id']]}\n",
    "                for i in range(branching_factor)\n",
    "            ],\n",
    "            'compact': False,\n",
    "            'max_gen_len': 512,\n",
    "        }\n",
    "\n",
    "        outputs, step1_metrics = measure_step(workflow, proposal_step_args)\n",
    "        (proposal_tokens, proposal_nodes) = get('tokens', 'nodes')(outputs)\n",
    "        \n",
    "        # Complete TTFT for proposal step\n",
    "        proposal_complete_ttft = time.time() - proposal_ttft_start\n",
    "        step1_metrics[\"complete_ttft\"] = proposal_complete_ttft\n",
    "        \n",
    "        baseline_step_metrics.append(step1_metrics)\n",
    "        total_tokens += step1_metrics[\"token_count\"]\n",
    "\n",
    "        # Start timing complete TTFT for vote step (including insert)\n",
    "        vote_ttft_start = time.time()\n",
    "        \n",
    "        vote_user_prompt = f'{format_problem(problem)}\\n\\nHere are the proposals:'\n",
    "        for i, prop in enumerate(proposal_tokens):\n",
    "            vote_user_prompt += f'\\n\\nSolution #{i+1}:\\n{workflow.tokenizer.decode(prop)}'\n",
    "\n",
    "        [vote] = workflow.insert([\n",
    "            {'messages': [\n",
    "                {'role': 'system', 'content': format_vote_system_prompt(branching_factor)},\n",
    "                {'role': 'user', 'content': vote_user_prompt}\n",
    "            ], 'parent_ids': []}\n",
    "        ])\n",
    "\n",
    "        vote_step_args = {\n",
    "            'tasks': [\n",
    "                {'header': ('assistant', None),\n",
    "                'prefill': 'BEST CHOICE: ',\n",
    "                'parent_ids': [vote['id']]}\n",
    "                for _ in range(voters)\n",
    "            ],\n",
    "            'compact': False,\n",
    "            'max_gen_len': 256,\n",
    "        }\n",
    "\n",
    "        outputs, step2_metrics = measure_step(workflow, vote_step_args)\n",
    "        vote_tokens = get('tokens')(outputs)\n",
    "        \n",
    "        # Complete TTFT for vote step\n",
    "        vote_complete_ttft = time.time() - vote_ttft_start\n",
    "        step2_metrics[\"complete_ttft\"] = vote_complete_ttft\n",
    "        \n",
    "        baseline_step_metrics.append(step2_metrics)\n",
    "        total_tokens += step2_metrics[\"token_count\"]\n",
    "\n",
    "        votes = [choice for v in vote_tokens if (choice := parse_choice(workflow.tokenizer.decode(v))) is not None]\n",
    "        final_tokens = None\n",
    "\n",
    "        if votes:\n",
    "            # Start timing complete TTFT for final step (including insert)\n",
    "            final_ttft_start = time.time()\n",
    "            \n",
    "            best = Counter(votes).most_common(1)[0][0]\n",
    "            best_proposal = workflow.tokenizer.decode(proposal_tokens[best - 1])\n",
    "            [finish] = workflow.insert([\n",
    "                {'messages': [\n",
    "                    {\"role\": \"system\", \"content\": finish_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"{format_problem(problem)}\\n\\nHere is the proposed approach: {best_proposal}\"}],\n",
    "                'parent_ids': []\n",
    "            }])\n",
    "\n",
    "            final_step_args = {\n",
    "                'tasks': [\n",
    "                    {'header': ('assistant', None),\n",
    "                    'prefill': '',\n",
    "                    'parent_ids': [finish['id']]}\n",
    "                ],\n",
    "                'max_gen_len': 256,\n",
    "            }\n",
    "\n",
    "            outputs, step3_metrics = measure_step(workflow, final_step_args)\n",
    "            final_tokens = get('tokens')(outputs)[0]\n",
    "            \n",
    "            # Complete TTFT for final step\n",
    "            final_complete_ttft = time.time() - final_ttft_start\n",
    "            step3_metrics[\"complete_ttft\"] = final_complete_ttft\n",
    "            \n",
    "            baseline_step_metrics.append(step3_metrics)\n",
    "            total_tokens += step3_metrics[\"token_count\"]\n",
    "\n",
    "        cuda_end_event.record(stream=None)\n",
    "        torch.cuda.synchronize()\n",
    "        baseline_cuda_time = cuda_start_event.elapsed_time(cuda_end_event) / 1000\n",
    "\n",
    "        baseline_end = time.time()\n",
    "        baseline_latency = baseline_end - baseline_start\n",
    "        baseline_tps = total_tokens / baseline_latency\n",
    "        baseline_ttft_avg = statistics.mean([m[\"complete_ttft\"] for m in baseline_step_metrics])\n",
    "\n",
    "        # Prepare teacher forcing tensors\n",
    "        proposal_force = torch.full((branching_factor, 512), workflow.tokenizer.eot_id, device=workflow.device)\n",
    "        for i, tokens in enumerate(proposal_tokens):\n",
    "            proposal_force[i, :len(tokens)] = torch.tensor(tokens, device=workflow.device)\n",
    "\n",
    "        voter_force = torch.full((voters, 256), workflow.tokenizer.eot_id, device=workflow.device)\n",
    "        for i, tokens in enumerate(vote_tokens):\n",
    "            voter_force[i, :len(tokens)] = torch.tensor(tokens, device=workflow.device)\n",
    "\n",
    "        final_force = None\n",
    "        if final_tokens is not None:\n",
    "            final_force = torch.full((1, 256), workflow.tokenizer.eot_id, device=workflow.device)\n",
    "            final_force[0, :len(final_tokens)] = torch.tensor(final_tokens, device=workflow.device)\n",
    "\n",
    "        # ====== CACHED ======\n",
    "        workflow.reset()\n",
    "        cached_start = time.time()\n",
    "        cuda_start_event = torch.cuda.Event(enable_timing=True)\n",
    "        cuda_end_event = torch.cuda.Event(enable_timing=True)\n",
    "        cuda_start_event.record(stream=None)\n",
    "\n",
    "        cached_step_metrics = []\n",
    "\n",
    "        # For cached implementation, insert all contexts upfront\n",
    "        insert_start = time.time()\n",
    "        cot, vote, finish = workflow.insert([\n",
    "            {'messages': [\n",
    "                {'role': 'system', 'content': cot_prompt},\n",
    "                {'role': 'user', 'content': format_problem(problem)}\n",
    "            ], 'parent_ids': []},\n",
    "            {'messages': [\n",
    "                {'role': 'system', 'content': format_vote_system_prompt(branching_factor)},\n",
    "                {'role': 'user', 'content': format_problem(problem)}\n",
    "            ], 'parent_ids': []},\n",
    "            {'messages': [\n",
    "                {'role': 'system', 'content': finish_prompt},\n",
    "                {'role': 'user', 'content': format_problem(problem)}\n",
    "            ], 'parent_ids': []},\n",
    "        ])\n",
    "        insert_time = time.time() - insert_start\n",
    "\n",
    "        # Start timing complete TTFT for proposal step\n",
    "        proposal_ttft_start = time.time()\n",
    "        \n",
    "        cached_proposal_step_args = {\n",
    "            'tasks': [\n",
    "                {'header': ('assistant', None),\n",
    "                'prefill': f'Solution #{i+1}:\\n\\n',\n",
    "                'parent_ids': [cot['id']]}\n",
    "                for i in range(branching_factor)\n",
    "            ],\n",
    "            'teacher_force': proposal_force,\n",
    "            'compact': False,\n",
    "            'max_gen_len': 512,\n",
    "        }\n",
    "\n",
    "        outputs, cached_step1_metrics = measure_step(workflow, cached_proposal_step_args)\n",
    "        cached_proposal_tokens, cached_proposal_nodes = get('tokens', 'nodes')(outputs)\n",
    "        \n",
    "        # Complete TTFT for proposal step (only need to add initial insert time to first step)\n",
    "        cached_step1_metrics[\"complete_ttft\"] = (time.time() - proposal_ttft_start) + insert_time\n",
    "        \n",
    "        cached_step_metrics.append(cached_step1_metrics)\n",
    "\n",
    "        # Start timing complete TTFT for vote step\n",
    "        vote_ttft_start = time.time()\n",
    "        \n",
    "        cached_vote_step_args = {\n",
    "            'tasks': [\n",
    "                {'header': ('assistant', None),\n",
    "                'prefill': 'BEST CHOICE: ',\n",
    "                'parent_ids': [vote['id']] + [p['id'] for p in cached_proposal_nodes]}\n",
    "                for _ in range(voters)\n",
    "            ],\n",
    "            'teacher_force': voter_force,\n",
    "            'stateless': False,\n",
    "            'compact': False,\n",
    "            'max_gen_len': 256,\n",
    "        }\n",
    "\n",
    "        outputs, cached_step2_metrics = measure_step(workflow, cached_vote_step_args)\n",
    "        cached_vote_tokens, cached_vote_nodes = get('tokens', 'nodes')(outputs)\n",
    "        \n",
    "        # Complete TTFT for vote step\n",
    "        cached_step2_metrics[\"complete_ttft\"] = time.time() - vote_ttft_start\n",
    "        \n",
    "        cached_step_metrics.append(cached_step2_metrics)\n",
    "\n",
    "        if final_force is not None and votes:\n",
    "            # Start timing complete TTFT for final step\n",
    "            final_ttft_start = time.time()\n",
    "            \n",
    "            best = Counter(votes).most_common(1)[0][0]\n",
    "            cached_final_step_args = {\n",
    "                'tasks': [\n",
    "                    {'header': ('assistant', None),\n",
    "                    'prefill': 'ANSWER: ',\n",
    "                    'parent_ids': [finish['id']] + [cached_proposal_nodes[best-1]['id']]}\n",
    "                ],\n",
    "                'teacher_force': final_force,\n",
    "                'stateless': False,\n",
    "                'compact': False,\n",
    "                'max_gen_len': 256,\n",
    "            }\n",
    "\n",
    "            outputs, cached_step3_metrics = measure_step(workflow, cached_final_step_args)\n",
    "            cached_final_tokens = get('tokens')(outputs)\n",
    "            \n",
    "            # Complete TTFT for final step\n",
    "            cached_step3_metrics[\"complete_ttft\"] = time.time() - final_ttft_start\n",
    "            \n",
    "            cached_step_metrics.append(cached_step3_metrics)\n",
    "\n",
    "        cuda_end_event.record(stream=None)\n",
    "        torch.cuda.synchronize()\n",
    "        cached_cuda_time = cuda_start_event.elapsed_time(cuda_end_event) / 1000\n",
    "\n",
    "        cached_end = time.time()\n",
    "        cached_latency = cached_end - cached_start\n",
    "        cached_tps = total_tokens / cached_latency\n",
    "        cached_ttft_avg = statistics.mean([m[\"complete_ttft\"] for m in cached_step_metrics])\n",
    "\n",
    "        metrics[\"baseline\"][\"tps\"].append(baseline_tps)\n",
    "        metrics[\"baseline\"][\"latency\"].append(baseline_latency)\n",
    "        metrics[\"baseline\"][\"cuda_latency\"].append(baseline_cuda_time)\n",
    "        metrics[\"baseline\"][\"ttft\"].append(baseline_ttft_avg)\n",
    "        metrics[\"baseline\"][\"steps\"].append(baseline_step_metrics)\n",
    "\n",
    "        metrics[\"cached\"][\"tps\"].append(cached_tps)\n",
    "        metrics[\"cached\"][\"latency\"].append(cached_latency)\n",
    "        metrics[\"cached\"][\"cuda_latency\"].append(cached_cuda_time)\n",
    "        metrics[\"cached\"][\"ttft\"].append(cached_ttft_avg)\n",
    "        metrics[\"cached\"][\"steps\"].append(cached_step_metrics)\n",
    "\n",
    "        print(f\"  Tokens generated: {total_tokens}\")\n",
    "        print(f\"  Baseline: TPS={baseline_tps:.1f}, Wall latency={baseline_latency:.3f}s, CUDA latency={baseline_cuda_time:.3f}s, Avg TTFT={baseline_ttft_avg*1000:.1f}ms\")\n",
    "        print(f\"  Cached: TPS={cached_tps:.1f}, Wall latency={cached_latency:.3f}s, CUDA latency={cached_cuda_time:.3f}s, Avg TTFT={cached_ttft_avg*1000:.1f}ms\")\n",
    "        print(f\"  Speedup: TPS={cached_tps/baseline_tps:.2f}x, Wall={baseline_latency/cached_latency:.2f}x, CUDA={baseline_cuda_time/cached_cuda_time:.2f}x, TTFT={baseline_ttft_avg/cached_ttft_avg:.2f}x\")\n",
    "\n",
    "        print(\"  Per-step complete TTFT improvement:\")\n",
    "        for i, (b_step, c_step) in enumerate(zip(baseline_step_metrics, cached_step_metrics)):\n",
    "            step_name = [\"proposals\", \"voting\", \"final\"][i]\n",
    "            ttft_improvement = b_step[\"complete_ttft\"] / c_step[\"complete_ttft\"] if c_step[\"complete_ttft\"] > 0 else float('inf')\n",
    "            print(f\"    {step_name}: {b_step['complete_ttft']*1000:.1f}ms → {c_step['complete_ttft']*1000:.1f}ms ({ttft_improvement:.2f}x)\")\n",
    "\n",
    "    summary = {}\n",
    "    for impl in [\"baseline\", \"cached\"]:\n",
    "        summary[impl] = {}\n",
    "        for metric in [\"tps\", \"latency\", \"cuda_latency\", \"ttft\"]:\n",
    "            values = metrics[impl][metric]\n",
    "            summary[impl][metric] = {\n",
    "                \"mean\": statistics.mean(values),\n",
    "                \"stdev\": statistics.stdev(values) if len(values) > 1 else 0\n",
    "            }\n",
    "\n",
    "    improvement = {\n",
    "        \"tps\": summary[\"cached\"][\"tps\"][\"mean\"] / summary[\"baseline\"][\"tps\"][\"mean\"],\n",
    "        \"latency\": summary[\"baseline\"][\"latency\"][\"mean\"] / summary[\"cached\"][\"latency\"][\"mean\"],\n",
    "        \"cuda_latency\": summary[\"baseline\"][\"cuda_latency\"][\"mean\"] / summary[\"cached\"][\"cuda_latency\"][\"mean\"],\n",
    "        \"ttft\": summary[\"baseline\"][\"ttft\"][\"mean\"] / summary[\"cached\"][\"ttft\"][\"mean\"]\n",
    "    }\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for impl in [\"baseline\", \"cached\"]:\n",
    "        print(f\"{impl.capitalize()}:\")\n",
    "        print(f\"  TPS: {summary[impl]['tps']['mean']:.2f} ± {summary[impl]['tps']['stdev']:.2f}\")\n",
    "        print(f\"  Wall Latency: {summary[impl]['latency']['mean']:.3f}s ± {summary[impl]['latency']['stdev']:.3f}s\")\n",
    "        print(f\"  CUDA Latency: {summary[impl]['cuda_latency']['mean']:.3f}s ± {summary[impl]['cuda_latency']['stdev']:.3f}s\")\n",
    "        print(f\"  Avg TTFT: {summary[impl]['ttft']['mean']*1000:.1f}ms ± {summary[impl]['ttft']['stdev']*1000:.1f}ms\")\n",
    "\n",
    "    print(\"\\nImprovement Ratios:\")\n",
    "    print(f\"  TPS: {improvement['tps']:.2f}x\")\n",
    "    print(f\"  Wall Latency: {improvement['latency']:.2f}x\")\n",
    "    print(f\"  CUDA Latency: {improvement['cuda_latency']:.2f}x\")\n",
    "    print(f\"  Avg TTFT: {improvement['ttft']:.2f}x\")\n",
    "\n",
    "    step_improvements = defaultdict(list)\n",
    "    for run in range(num_runs):\n",
    "        for i, (step_name, b_step, c_step) in enumerate([\n",
    "            (name, b, c)\n",
    "            for name, b, c in zip(\n",
    "                [\"proposals\", \"voting\", \"final\"],\n",
    "                metrics[\"baseline\"][\"steps\"][run],\n",
    "                metrics[\"cached\"][\"steps\"][run]\n",
    "            )\n",
    "            if len(metrics[\"baseline\"][\"steps\"][run]) > i and len(metrics[\"cached\"][\"steps\"][run]) > i\n",
    "        ]):\n",
    "            step_improvements[f\"{step_name}_ttft\"].append(b_step[\"complete_ttft\"] / c_step[\"complete_ttft\"])\n",
    "\n",
    "    print(\"\\nPer-step TTFT Improvements:\")\n",
    "    for step_name, values in step_improvements.items():\n",
    "        mean = statistics.mean(values)\n",
    "        stdev = statistics.stdev(values) if len(values) > 1 else 0\n",
    "        print(f\"  {step_name}: {mean:.2f}x ± {stdev:.2f}x\")\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"improvement\": improvement,\n",
    "        \"raw\": metrics,\n",
    "        \"step_improvements\": {k: {\n",
    "            \"mean\": statistics.mean(v),\n",
    "            \"stdev\": statistics.stdev(v) if len(v) > 1 else 0\n",
    "        } for k, v in step_improvements.items()}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42840470-2627-418c-b5a5-abb3252a6baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/3\n",
      "  Tokens generated: 1281\n",
      "  Baseline: TPS=79.9, Wall latency=16.037s, CUDA latency=16.037s, Avg TTFT=5345.6ms\n",
      "  Cached: TPS=80.4, Wall latency=15.931s, CUDA latency=15.930s, Avg TTFT=5310.1ms\n",
      "  Speedup: TPS=1.01x, Wall=1.01x, CUDA=1.01x, TTFT=1.01x\n",
      "  Per-step complete TTFT improvement:\n",
      "    proposals: 6489.3ms → 6358.9ms (1.02x)\n",
      "    voting: 1931.4ms → 2008.7ms (0.96x)\n",
      "    final: 7616.1ms → 7562.8ms (1.01x)\n",
      "Run 2/3\n",
      "  Tokens generated: 1281\n",
      "  Baseline: TPS=79.8, Wall latency=16.050s, CUDA latency=16.050s, Avg TTFT=5349.8ms\n",
      "  Cached: TPS=80.4, Wall latency=15.935s, CUDA latency=15.935s, Avg TTFT=5311.8ms\n",
      "  Speedup: TPS=1.01x, Wall=1.01x, CUDA=1.01x, TTFT=1.01x\n",
      "  Per-step complete TTFT improvement:\n",
      "    proposals: 6489.7ms → 6354.3ms (1.02x)\n",
      "    voting: 1936.5ms → 2010.3ms (0.96x)\n",
      "    final: 7623.3ms → 7570.7ms (1.01x)\n",
      "Run 3/3\n",
      "  Tokens generated: 1281\n",
      "  Baseline: TPS=79.9, Wall latency=16.026s, CUDA latency=16.025s, Avg TTFT=5341.8ms\n",
      "  Cached: TPS=80.3, Wall latency=15.943s, CUDA latency=15.943s, Avg TTFT=5314.4ms\n",
      "  Speedup: TPS=1.01x, Wall=1.01x, CUDA=1.01x, TTFT=1.01x\n",
      "  Per-step complete TTFT improvement:\n",
      "    proposals: 6480.9ms → 6356.1ms (1.02x)\n",
      "    voting: 1932.1ms → 2008.9ms (0.96x)\n",
      "    final: 7612.4ms → 7578.3ms (1.00x)\n",
      "\n",
      "Final Results:\n",
      "Baseline:\n",
      "  TPS: 79.88 ± 0.06\n",
      "  Wall Latency: 16.037s ± 0.012s\n",
      "  CUDA Latency: 16.037s ± 0.012s\n",
      "  Avg TTFT: 5345.7ms ± 4.0ms\n",
      "Cached:\n",
      "  TPS: 80.38 ± 0.03\n",
      "  Wall Latency: 15.936s ± 0.006s\n",
      "  CUDA Latency: 15.936s ± 0.006s\n",
      "  Avg TTFT: 5312.1ms ± 2.2ms\n",
      "\n",
      "Improvement Ratios:\n",
      "  TPS: 1.01x\n",
      "  Wall Latency: 1.01x\n",
      "  CUDA Latency: 1.01x\n",
      "  Avg TTFT: 1.01x\n",
      "\n",
      "Per-step TTFT Improvements:\n",
      "  proposals_ttft: 1.02x ± 0.00x\n",
      "  voting_ttft: 0.96x ± 0.00x\n",
      "  final_ttft: 1.01x ± 0.00x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'baseline': {'tps': {'mean': 79.87575043165356,\n",
       "    'stdev': 0.06013996787366561},\n",
       "   'latency': {'mean': 16.037414073944092, 'stdev': 0.012075318023978323},\n",
       "   'cuda_latency': {'mean': 16.037323567708334, 'stdev': 0.012077594121968668},\n",
       "   'ttft': {'mean': 5.345736026763916, 'stdev': 0.004023550558760566}},\n",
       "  'cached': {'tps': {'mean': 80.38205126479731, 'stdev': 0.03251895356200607},\n",
       "   'latency': {'mean': 15.936395247777304, 'stdev': 0.006447758699455069},\n",
       "   'cuda_latency': {'mean': 15.936302408854166, 'stdev': 0.006440445112084791},\n",
       "   'ttft': {'mean': 5.312100887298584, 'stdev': 0.0021524800112217167}}},\n",
       " 'improvement': {'tps': 1.006338605026027,\n",
       "  'latency': 1.0063388755484637,\n",
       "  'cuda_latency': 1.0063390588520735,\n",
       "  'ttft': 1.0063317960593998},\n",
       " 'raw': {'baseline': {'tps': [79.87769860795333,\n",
       "    79.81466004627343,\n",
       "    79.9348926407339],\n",
       "   'latency': [16.03701686859131, 16.049683094024658, 16.02554225921631],\n",
       "   'cuda_latency': [16.03694921875, 16.049583984375, 16.0254375],\n",
       "   'ttft': [5.345612208048503, 5.349820057551066, 5.34177581469218],\n",
       "   'steps': [[{'wall_time': 6.447981119155884,\n",
       "      'cuda_time': 6.446859375,\n",
       "      'token_count': 928,\n",
       "      'ttft': 6.447980642318726,\n",
       "      'complete_ttft': 6.489348649978638},\n",
       "     {'wall_time': 1.8895540237426758,\n",
       "      'cuda_time': 1.80053173828125,\n",
       "      'token_count': 165,\n",
       "      'ttft': 1.8895533084869385,\n",
       "      'complete_ttft': 1.9313554763793945},\n",
       "     {'wall_time': 7.574968576431274,\n",
       "      'cuda_time': 7.56847509765625,\n",
       "      'token_count': 188,\n",
       "      'ttft': 7.574967622756958,\n",
       "      'complete_ttft': 7.616132497787476}],\n",
       "    [{'wall_time': 6.448349952697754,\n",
       "      'cuda_time': 6.44716455078125,\n",
       "      'token_count': 928,\n",
       "      'ttft': 6.448349475860596,\n",
       "      'complete_ttft': 6.489673376083374},\n",
       "     {'wall_time': 1.8944206237792969,\n",
       "      'cuda_time': 1.806257568359375,\n",
       "      'token_count': 165,\n",
       "      'ttft': 1.8944201469421387,\n",
       "      'complete_ttft': 1.936492681503296},\n",
       "     {'wall_time': 7.582031488418579,\n",
       "      'cuda_time': 7.5757080078125,\n",
       "      'token_count': 188,\n",
       "      'ttft': 7.58203125,\n",
       "      'complete_ttft': 7.623294115066528}],\n",
       "    [{'wall_time': 6.439685344696045,\n",
       "      'cuda_time': 6.43852587890625,\n",
       "      'token_count': 928,\n",
       "      'ttft': 6.439683675765991,\n",
       "      'complete_ttft': 6.480909109115601},\n",
       "     {'wall_time': 1.890162706375122,\n",
       "      'cuda_time': 1.801470703125,\n",
       "      'token_count': 165,\n",
       "      'ttft': 1.890162467956543,\n",
       "      'complete_ttft': 1.9320580959320068},\n",
       "     {'wall_time': 7.571020841598511,\n",
       "      'cuda_time': 7.56456103515625,\n",
       "      'token_count': 188,\n",
       "      'ttft': 7.5710203647613525,\n",
       "      'complete_ttft': 7.612360239028931}]]},\n",
       "  'cached': {'tps': [80.4116477489864, 80.38726597934344, 80.34724006606208],\n",
       "   'latency': [15.930527925491333, 15.935359716415405, 15.943298101425171],\n",
       "   'cuda_latency': [15.9304462890625, 15.9352607421875, 15.9432001953125],\n",
       "   'ttft': [5.310143073399861, 5.311753749847412, 5.3144058386484785],\n",
       "   'steps': [[{'wall_time': 6.3181092739105225,\n",
       "      'cuda_time': 6.300041015625,\n",
       "      'token_count': 960,\n",
       "      'ttft': 6.318108558654785,\n",
       "      'complete_ttft': 6.358933448791504},\n",
       "     {'wall_time': 2.008495569229126,\n",
       "      'cuda_time': 2.0085390625,\n",
       "      'token_count': 185,\n",
       "      'ttft': 2.0084950923919678,\n",
       "      'complete_ttft': 2.00872802734375},\n",
       "     {'wall_time': 7.562523126602173,\n",
       "      'cuda_time': 7.56258935546875,\n",
       "      'token_count': 192,\n",
       "      'ttft': 7.562522649765015,\n",
       "      'complete_ttft': 7.562767744064331}],\n",
       "    [{'wall_time': 6.313199043273926,\n",
       "      'cuda_time': 6.2952783203125,\n",
       "      'token_count': 960,\n",
       "      'ttft': 6.313198566436768,\n",
       "      'complete_ttft': 6.354326963424683},\n",
       "     {'wall_time': 2.0099618434906006,\n",
       "      'cuda_time': 2.010011962890625,\n",
       "      'token_count': 185,\n",
       "      'ttft': 2.0099613666534424,\n",
       "      'complete_ttft': 2.010258197784424},\n",
       "     {'wall_time': 7.5703723430633545,\n",
       "      'cuda_time': 7.5704716796875,\n",
       "      'token_count': 192,\n",
       "      'ttft': 7.570371866226196,\n",
       "      'complete_ttft': 7.57067608833313}],\n",
       "    [{'wall_time': 6.314979553222656,\n",
       "      'cuda_time': 6.29679443359375,\n",
       "      'token_count': 960,\n",
       "      'ttft': 6.314979076385498,\n",
       "      'complete_ttft': 6.356078624725342},\n",
       "     {'wall_time': 2.008632183074951,\n",
       "      'cuda_time': 2.008671630859375,\n",
       "      'token_count': 185,\n",
       "      'ttft': 2.008631944656372,\n",
       "      'complete_ttft': 2.0088794231414795},\n",
       "     {'wall_time': 7.57798171043396,\n",
       "      'cuda_time': 7.57802001953125,\n",
       "      'token_count': 192,\n",
       "      'ttft': 7.577981472015381,\n",
       "      'complete_ttft': 7.578259468078613}]]}},\n",
       " 'step_improvements': {'proposals_ttft': {'mean': 1.0204827998499217,\n",
       "   'stdev': 0.0008304783743560354},\n",
       "  'voting_ttft': {'mean': 0.962182128370051, 'stdev': 0.0009826571262804614},\n",
       "  'final_ttft': {'mean': 1.006168767896366, 'stdev': 0.0014463264982470894}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.workflows.tot import load_math_problems\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='val')\n",
    "\n",
    "benchmark(\n",
    "    workflow,\n",
    "    problems[0]['problem'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637dfc35-39b4-4705-b2c2-95e564606b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_tot_transcript(result, tokenizer, problem: str) -> str:\n",
    "    transcript = []\n",
    "    \n",
    "    # Header\n",
    "    transcript.append(f\"# Tree of Thoughts Process for Problem:\\n{problem}\\n\")\n",
    "    \n",
    "    # Stage 1: Strategies\n",
    "    transcript.append(\"## Stage 1: Strategy Generation\")\n",
    "    for i, tokens in enumerate(result['strategy_tokens']):\n",
    "        transcript.append(f\"\\n### Strategy #{i+1}:\\n{tokenizer.decode(tokens)}\")\n",
    "    \n",
    "    # Strategy Votes\n",
    "    transcript.append(\"\\n## Strategy Voting\")\n",
    "    for i, tokens in enumerate(result['strategy_vote_tokens']):\n",
    "        transcript.append(f\"\\nVoter #{i+1}: {tokenizer.decode(tokens)}\")\n",
    "    \n",
    "    # Best Strategy\n",
    "    if result['strategy_votes']:\n",
    "        vote_counts = Counter(result['strategy_votes'])\n",
    "        best_strategy_idx = vote_counts.most_common(1)[0][0] - 1\n",
    "        best_strategy = tokenizer.decode(result['strategy_tokens'][best_strategy_idx])\n",
    "        \n",
    "        transcript.append(f\"\\n## Best Strategy (#{best_strategy_idx+1}):\\n{best_strategy}\")\n",
    "    else:\n",
    "        transcript.append(\"\\n## No valid strategy votes\")\n",
    "        return \"\\n\".join(transcript)\n",
    "    \n",
    "    # Stage 2: Solutions\n",
    "    transcript.append(\"\\n## Stage 2: Solution Generation\")\n",
    "    for i, tokens in enumerate(result['solution_tokens']):\n",
    "        transcript.append(f\"\\n### Solution #{i+1}:\\n{tokenizer.decode(tokens)}\")\n",
    "    \n",
    "    # Solution Votes\n",
    "    transcript.append(\"\\n## Solution Voting\")\n",
    "    for i, tokens in enumerate(result['solution_vote_tokens']):\n",
    "        transcript.append(f\"\\nVoter #{i+1}: {tokenizer.decode(tokens)}\")\n",
    "    \n",
    "    # Final Solution\n",
    "    if result['solution_votes']:\n",
    "        vote_counts = Counter(result['solution_votes'])\n",
    "        best_solution_idx = vote_counts.most_common(1)[0][0] - 1\n",
    "        transcript.append(f\"\\n## Final Selected Solution (#{best_solution_idx+1}):\")\n",
    "        if result['final_tokens'] is not None:\n",
    "            transcript.append(f\"\\n{tokenizer.decode(result['final_tokens'])}\")\n",
    "    else:\n",
    "        transcript.append(\"\\n## No valid solution votes\")\n",
    "    \n",
    "    return \"\\n\".join(transcript)\n",
    "\n",
    "print(create_tot_transcript(outputs, workflow.tokenizer, problems[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0ec7d40-2b46-4013-84c8-afde28f406e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:01<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from llama.workflows.tot import load_math_problems, eval_solutions\n",
    "\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='val')\n",
    "\n",
    "with open('/home/tbai4/llama3/dumps/mad/math_cached_e2e.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "llama.model.reshape_cache(4)\n",
    "results = eval_solutions(\n",
    "    llama=llama,\n",
    "    solutions=data,\n",
    "    problems=problems\n",
    ")\n",
    "\n",
    "print(sum(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df95f8-ab15-45a8-925f-296d72318672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.workflows.tot import tot_baseline, tot_baseline_shuffled, load_math_problems\n",
    "\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='val')\n",
    "\n",
    "workflow.reset()\n",
    "outputs = tot_baseline_shuffled(\n",
    "    workflow,\n",
    "    problems[0]['problem'],\n",
    "    8, 4,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd04232-8c5c-472f-a3ed-126fb7e70679",
   "metadata": {},
   "source": [
    "## generate fine-tuning examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2ee33-5f08-47da-8b25-479da0cd12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.workflows.tot import collect_samples\n",
    "\n",
    "samples = collect_samples(\n",
    "    llama=llama,\n",
    "    save_dir='/home/tbai4/llama3/dumps',\n",
    "    n_problems=1000,\n",
    "    branching_factor=8,\n",
    "    voters=4,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    seed=42,\n",
    "    math_path='/home/tbai4/llama3/data/MATH',\n",
    "    split='train',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
