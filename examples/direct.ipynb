{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c40cdc6-e4e2-4c42-8a9c-daf4e5742c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef75228-06cc-41e0-b613-708050ae79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.workflows.finetune import finetune\n",
    "\n",
    "finetune(\n",
    "    task='direct',\n",
    "    data_path='/home/tbai4/llama3/dumps/simple/from_mad.json',\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    output_dir='/scratch4/jeisner1/tjbai/checkpoints/direct/from_mad',\n",
    "    max_seq_len=8*8192,\n",
    "    epochs=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    checkpoint_freq=100,\n",
    "    validation_freq=100,\n",
    "    lora_rank=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    learning_rate=5e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2da18f3-de7e-4275-a465-0691682db344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbai4/llama3/llama/util.py:60: UserWarning: 16384 does not lie within [1, 8192]\n",
      "  warnings.warn(f\"{max_seq_len} does not lie within [1, 8192]\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Converting to LoRA\n",
      "Loaded in 10.63 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama import Workflow, Llama\n",
    "from llama.util import find_free_port, load_ckpt\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
    "\n",
    "workflow = Workflow.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=2*8192,\n",
    "    max_batch_size=1,\n",
    "    model_parallel_size=1,\n",
    "    max_nodes=100,\n",
    "    use_lora=True,\n",
    "    lora_rank=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "llama = Llama(workflow.model, workflow.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b52fbc-a558-43d7-995a-d4b5ec65c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:15<00:00,  2.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from llama.workflows.tot import load_math_problems, eval_solutions\n",
    "\n",
    "llama.model.reshape_cache(4)\n",
    "\n",
    "with open('/home/tbai4/llama3/dumps/math_baseline_direct_test.json') as f:\n",
    "    data = json.load(f)\n",
    "    print(len(data))\n",
    "\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='test')[:500]\n",
    "\n",
    "outputs = eval_solutions(\n",
    "    llama,\n",
    "    solutions=data,\n",
    "    problems=problems\n",
    ")\n",
    "\n",
    "sum(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6381f-1ee0-4a23-a95f-a2c55402c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ckpt(workflow, '/scratch4/jeisner1/tjbai/checkpoints/direct/from_madpar/lora_step-149.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2b8922-9f6c-405a-a8db-c6dd11c34c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.workflows.tot import load_math_problems\n",
    "\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "062c3a3f-19fd-4e57-9333-26fb757efce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: Based on the summary, it appears that all three agents agree that the correct answer is $\\\\boxed{2}$. This is because they all recognize that the statement \"At least one of the numbers $a, b,$ and $c$ is odd\" is equivalent to the statement \"At least one of $a, b,$ and $c$ is odd or all of $a, b,$ and $c$ are even\".\\n\\nThe first agent\\'s statement that the answer is $\\\\boxed{2}$ is correct, as it is the number of possibilities where all three numbers are even. The second agent\\'s statement that the answer is $\\\\boxed{3}$ is incorrect, as it is the number of possibilities where at least one of the numbers is odd, not all three.\\n\\nThe third agent\\'s statement that the answer is $\\\\boxed{2}$ is also correct, as it is the number of possibilities where all three numbers are even. This is because the statement \"At least one of the numbers $a, b,$ and $c$ is odd\" is equivalent to the statement \"At least one of $a, b,$ and $c$ is odd or all of $a, b,$ and $c$ are even\".\\n\\nTherefore, the correct answer is indeed $\\\\boxed{2}$, as all three agents agree.\\n\\nThe final answer is: $\\\\boxed{2}$'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama.workflows.simple import math_direct\n",
    "\n",
    "math_direct(\n",
    "    workflow,\n",
    "    problem=problems[1]['problem'],\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1753f57e-ac24-4548-86f7-92000914c636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Answer: Based on the summary, it's clear that all three agents agree on the final answer to the math problem, which is $441$.\\n\\nHere is the revised answer:\\n\\nThe final answer is: $\\\\boxed{441}$\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d561dc2-a50a-4edd-aa5d-edace9078e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 54.5M / 8.1B parameters\n"
     ]
    }
   ],
   "source": [
    "from llama.workflows.trainers import DirectTrainer, ListDataset\n",
    "\n",
    "dataset = ListDataset('/home/tbai4/llama3/dumps/simple/from_tot.json')\n",
    "\n",
    "trainer = DirectTrainer(workflow, '/scratch4/jeisner1/tjbai/checkpoints/simple/from_mad', 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49574945-21a2-443e-83cb-a629883dd5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:29,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8235, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    n = 0\n",
    "    for i, sample in tqdm(enumerate(dataset[:100])):\n",
    "        if len(sample['outputs']['solution']) == 0:\n",
    "            continue\n",
    "        n += 1\n",
    "        total += trainer.step(sample)[0].cpu()\n",
    "    print(total / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7f3a5-86e5-4159-a1fb-03508510d648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
