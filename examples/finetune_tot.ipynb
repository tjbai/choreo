{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37598c9a-64e3-4f91-8566-d0f76c1029c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tbai4/llama3/llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbai4/llama3/.venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%cd llama3/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99171e6f-6165-4597-89c2-20c906602ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Converting to LoRA\n",
      "Loaded in 18.76 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama import Workflow, Llama\n",
    "from llama.util import load_model_and_tokenizer\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "workflow = Workflow.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=8192,\n",
    "    max_batch_size=1,\n",
    "    model_parallel_size=1,\n",
    "    max_nodes=20,\n",
    "    use_lora=True,\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe0c8be-4c4f-43df-974a-0cc4d251e8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08480376642881861"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.model.get_trainable_param_percentage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de40b66f-721f-495c-a730-75130115a4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.reset()\n",
    "workflow.model.set_adapter_state(enabled=False)\n",
    "\n",
    "[system] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer the user\\'s question please.'}],\n",
    "        'parent_ids': [],\n",
    "    },\n",
    "])\n",
    "\n",
    "[user_1] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the capital of France?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "])\n",
    "\n",
    "[output], _ = workflow.step(\n",
    "    [\n",
    "        {\n",
    "            'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [system['id'], user_1['id']],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "workflow.tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24468385-fe66-42ba-a0bd-f9fd7cbb7418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.reset()\n",
    "workflow.model.set_adapter_state(enabled=True)\n",
    "\n",
    "[system] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer the user\\'s question please.'}],\n",
    "        'parent_ids': [],\n",
    "    },\n",
    "])\n",
    "\n",
    "[user_1] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the capital of France?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "])\n",
    "\n",
    "[output], _ = workflow.step(\n",
    "    [\n",
    "        {\n",
    "            'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [system['id'], user_1['id']],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "workflow.tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf5dca21-5719-4807-a901-12f505b8f590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['proposal_content', 'proposal_tokens', 'proposal_logprobs', 'vote_content', 'vote_tokens', 'vote_logprobs', 'final_content', 'final_tokens', 'final_logprobs', 'votes'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "problem = torch.load('tot_data/problem_0.pt', weights_only=True)\n",
    "\n",
    "problem['result'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "790de9c6-ca6b-41a6-80a8-501dd35a21f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 6.8M / 8.0B parameters\n"
     ]
    }
   ],
   "source": [
    "from llama.workflows.finetune import TotTrainer\n",
    "\n",
    "trainer = TotTrainer(workflow, branching_factor=8, voters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fda22972-941c-4c7f-bc05-8b133ee8ee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067\n",
      "153\n",
      "116\n",
      "torch.Size([1, 1075, 128256])\n",
      "torch.Size([1, 157, 128256])\n",
      "torch.Size([1, 117, 128256])\n"
     ]
    }
   ],
   "source": [
    "proposal_logprobs, vote_logprobs, final_logprobs = trainer.step(problem)\n",
    "\n",
    "print(sum(len(p) for p in problem['result']['proposal_tokens']))  # should be off by 8\n",
    "print(sum(len(v) for v in problem['result']['vote_tokens']))      # should be off by 4\n",
    "print(len(problem['result']['final_tokens']))                     # should be off by 1\n",
    "\n",
    "print(proposal_logprobs.shape)\n",
    "print(vote_logprobs.shape)\n",
    "print(final_logprobs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
