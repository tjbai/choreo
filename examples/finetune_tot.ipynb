{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37598c9a-64e3-4f91-8566-d0f76c1029c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tbai4/llama3/llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbai4/llama3/.venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%cd llama3/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99171e6f-6165-4597-89c2-20c906602ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbai4/llama3/llama/util.py:55: UserWarning: 12288 does not lie within [1, 8192]\n",
      "  warnings.warn(f\"{max_seq_len} does not lie within [1, 8192]\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama import Workflow, Llama\n",
    "from llama.util import load_model_and_tokenizer\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "workflow = Workflow.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=12288,\n",
    "    max_batch_size=2,\n",
    "    model_parallel_size=1,\n",
    "    max_nodes=20,\n",
    "    use_lora=True,\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "workflow.model.get_trainable_param_percentage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e46ac587-8dc3-4a52-bceb-eb8f4226d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 6.8M / 8.0B parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama.workflows.finetune import TotTrainer\n",
    "from llama.workflows.tot import cot_prompt, finish_prompt, format_vote_system_prompt, format_problem\n",
    "\n",
    "sample = torch.load('tot_data/problem_0.pt', weights_only=True)\n",
    "trainer = TotTrainer(workflow, branching_factor=8, voters=4)\n",
    "\n",
    "cot, vote, finish = workflow.insert([\n",
    "    {'messages': [\n",
    "        {'role': 'system', 'content': cot_prompt},\n",
    "        {'role': 'user', 'content': format_problem(sample['problem'])}\n",
    "    ], 'parent_ids': []},\n",
    "    {'messages': [\n",
    "        {'role': 'system', 'content': format_vote_system_prompt(8)},\n",
    "        {'role': 'user', 'content': format_problem(sample['problem'])}\n",
    "    ], 'parent_ids': []},\n",
    "    {'messages': [\n",
    "        {'role': 'system', 'content': finish_prompt},\n",
    "        {'role': 'user', 'content': format_problem(sample['problem'])}\n",
    "    ], 'parent_ids': []},\n",
    "], training=True)\n",
    "\n",
    "proposal_tasks = [\n",
    "    {'header': ('assistant', None),\n",
    "     'prefill': f'Solution #{i+1}:\\n\\n',\n",
    "     'parent_ids': [cot['id']]}\n",
    "    for i in range(8)\n",
    "]\n",
    "target_proposal_ids = [p + [workflow.tokenizer.eot_id] for p in sample['result']['proposal_tokens']]\n",
    "proposal_nodes, proposal_logprobs = workflow.train_step(proposal_tasks, target_proposal_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de40b66f-721f-495c-a730-75130115a4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.reset()\n",
    "workflow.model.set_adapter_state(enabled=False)\n",
    "\n",
    "[system] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer the user\\'s question please.'}],\n",
    "        'parent_ids': [],\n",
    "    },\n",
    "])\n",
    "\n",
    "[user_1] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the capital of France?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "])\n",
    "\n",
    "[output], _ = workflow.step(\n",
    "    [\n",
    "        {\n",
    "            'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [system['id'], user_1['id']],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "workflow.tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24468385-fe66-42ba-a0bd-f9fd7cbb7418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.reset()\n",
    "workflow.model.set_adapter_state(enabled=True)\n",
    "\n",
    "[system] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer the user\\'s question please.'}],\n",
    "        'parent_ids': [],\n",
    "    },\n",
    "])\n",
    "\n",
    "[user_1] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the capital of France?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "])\n",
    "\n",
    "[output], _ = workflow.step(\n",
    "    [\n",
    "        {\n",
    "            'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [system['id'], user_1['id']],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "workflow.tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6ac29-8c09-446b-b0ef-46d7803e1d5b",
   "metadata": {},
   "source": [
    "## sanity check forward-backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d67520-94c1-407f-a9ba-4ab77f476b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 6.8M / 8.0B parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama.workflows.finetune import TotTrainer\n",
    "from llama.workflows.tot import cot_prompt, finish_prompt, format_vote_system_prompt, format_problem\n",
    "\n",
    "problem = torch.load('tot_data/problem_0.pt', weights_only=True)\n",
    "trainer = TotTrainer(workflow, branching_factor=8, voters=4)\n",
    "\n",
    "workflow.model.train()\n",
    "workflow.model.set_adapter_state(enabled=True)\n",
    "workflow.model.zero_grad()\n",
    "\n",
    "total_loss, metrics = trainer.step(problem)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e555c4d9-c44c-4b75-9f53-366e5bc0c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.workflows.finetune import finetune\n",
    "\n",
    "finetune(\n",
    "    data_path='/home/tbai4/llama3/llama/tot_data_2',\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    output_dir='/scratch4/jeisner1/tjbai/checkpoints/tot_2',\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    max_seq_len=6144,\n",
    "    checkpoint_freq=100,\n",
    "    validation_freq=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf32633-2499-4090-915f-0c3f3cc33d33",
   "metadata": {},
   "source": [
    "## load and run checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544edab-2825-4bc2-9e4b-7b2ddef5da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load('/scratch4/jeisner1/tjbai/checkpoints/lora_epoch-0_step-399.pt', weights_only=True)\n",
    "workflow.model.load_state_dict(checkpoint['lora'])\n",
    "\n",
    "workflow.reset()\n",
    "workflow.model.eval()\n",
    "workflow.model.set_adapter_state(True)\n",
    "\n",
    "[system] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer ALL of the user\\'s question(s).'}],\n",
    "        'parent_ids': [],\n",
    "    },\n",
    "])\n",
    "\n",
    "[user_1, user_2] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the capital of France?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the largest planet in the solar system?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "])\n",
    "\n",
    "[output], _ = workflow.step(\n",
    "    [\n",
    "        {\n",
    "            'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [system['id'], user_1['id'], user_2['id']],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "workflow.tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a8cdf-36c3-4847-b078-656f02b9b904",
   "metadata": {},
   "source": [
    "## evaluate trick prompt results across checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942a2e8-9724-4e46-bb4b-e5ff3ba1df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "import random\n",
    "from llama import Llama\n",
    "from llama.workflows.tot import load_math_problems, benchmark_tricky_tot\n",
    "from tqdm import tqdm\n",
    "\n",
    "problems = load_math_problems('../data/MATH', split='val')\n",
    "problems = random.sample(problems, 200)\n",
    "\n",
    "for id in [99, 199, 299, 399]: \n",
    "    checkpoint = torch.load(f'/scratch4/jeisner1/tjbai/checkpoints/lora_epoch-0_step-{id}.pt', weights_only=True)\n",
    "    workflow.model.load_state_dict(checkpoint['lora'])\n",
    "    llama = Llama(workflow.model, workflow.tokenizer)\n",
    "    print(f'Loaded checkpoint-{id}')\n",
    "    print(f'Memory allocated: {torch.cuda.memory_allocated()}')\n",
    "\n",
    "    comps = []\n",
    "    for problem in tqdm(problems):\n",
    "        comps.append(benchmark_tricky_tot(\n",
    "            llama=llama,\n",
    "            workflow=workflow,\n",
    "            problem=problem['problem'],\n",
    "            branching_factor=8,\n",
    "            voters=4\n",
    "        ))\n",
    "        \n",
    "    with open(f'checkpoint-{id}_trick_results.json', 'w') as f:\n",
    "        json.dump(comps, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382050c0-f3ef-430e-9fc8-ae265edad973",
   "metadata": {},
   "source": [
    "## generate and evaluate final solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a7f71-f8f9-4411-8606-9fdcf12e2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "from llama import Llama\n",
    "from llama.workflows.tot import load_math_problems, benchmark_solution_quality\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "problems = load_math_problems('../data/MATH', split='val')\n",
    "problems = random.sample(problems, 200)\n",
    "\n",
    "for id in [99, 199, 299, 399]: \n",
    "    checkpoint = torch.load(f'/scratch4/jeisner1/tjbai/checkpoints/lora_epoch-0_step-{id}.pt', weights_only=True)\n",
    "    workflow.model.load_state_dict(checkpoint['lora'])\n",
    "    llama = Llama(workflow.model, workflow.tokenizer)\n",
    "    print(f'Loaded checkpoint-{id}')\n",
    "    print(f'Memory allocated: {torch.cuda.memory_allocated()}')\n",
    "\n",
    "    comps = []\n",
    "    for problem in tqdm(problems):\n",
    "        comps.append(benchmark_solution_quality(\n",
    "            llama=llama,\n",
    "            workflow=workflow,\n",
    "            problem=problem['problem'],\n",
    "            branching_factor=8,\n",
    "            voters=4,\n",
    "            compact=False,\n",
    "        ))\n",
    "        \n",
    "    with open(f'checkpoint-{id}_solution_quality.json', 'w') as f:\n",
    "        json.dump(comps, f)\n",
    "        \n",
    "# TODO -- get the untrained version's results too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8caef2ef-03ee-4845-a585-c49608ca4f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded in 13.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [13:22<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [13:23<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [14:00<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [13:52<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [13:53<00:00,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama import Llama\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "llama = Llama.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=8192,\n",
    "    max_batch_size=4,\n",
    "    model_parallel_size=1,\n",
    ")\n",
    "\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from llama.workflows.tot import load_math_problems, benchmark_solution_quality, parse_choice\n",
    "\n",
    "evaluator_prompt = '''\n",
    "You are evaluating final answers to AMC/AIME competition problems. You will receive:\n",
    "\n",
    "1. A problem statement\n",
    "2. The ground truth solution \n",
    "3. A shared solution proposal that both contestants used\n",
    "4. Two final answers based on this proposal\n",
    "\n",
    "Your task is to evaluate how effectively each contestant converted the shared proposal into a valid solution.\n",
    "Note that valid solutions may differ from the ground truth approach while remaining correct.\n",
    "\n",
    "Evaluate both answers focusing on:\n",
    "1. Answer Format Quality\n",
    "- Clarity and conciseness of final statement\n",
    "- Proper mathematical notation\n",
    "- Inclusion of key numerical result\n",
    "\n",
    "2. Mathematical Validity\n",
    "- Correctness of final numerical answer\n",
    "- Completeness (all parts answered)\n",
    "- Any invalid mathematical claims\n",
    "\n",
    "3. Justification Level\n",
    "- Appropriate amount of supporting context\n",
    "- Balance between brevity and explanation\n",
    "- Clear connection to previous reasoning\n",
    "\n",
    "Walk through each of these criterion and compare the 2 solutions. \n",
    "\n",
    "You must format your response as:\n",
    "\n",
    "VERDICT: [1 or 2]\n",
    "VERDICT_NOTE: (one sentence explanation)\n",
    "'''\n",
    "\n",
    "random.seed(42)\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='val')\n",
    "problems = random.sample(problems, 200)\n",
    "\n",
    "for id in [0, 99, 199, 299, 399]:\n",
    "    with open(f'/home/tbai4/llama3/dumps/checkpoint-{id}_solution_quality.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    baseline_win = 0\n",
    "    cached_win = 0\n",
    "\n",
    "    for d, problem_obj in tqdm(zip(data, problems), total=200):\n",
    "        problem = d['problem']\n",
    "        solution = problem_obj['solution']\n",
    "        baseline_final = d['baseline_final']\n",
    "        cached_final = d['cached_final']\n",
    "\n",
    "        votes = [\n",
    "            choice for resp in d['voters'] if\n",
    "            (choice := parse_choice(resp)) is not None\n",
    "        ]\n",
    "        best = Counter(votes).most_common(1)[0][0] - 1\n",
    "\n",
    "        baseline_first = random.choice([True, False])\n",
    "        ans1 = baseline_final if baseline_first else cached_final \n",
    "        ans2 = cached_final if baseline_first else baseline_final\n",
    "\n",
    "        dialog = [\n",
    "            {'role': 'system', 'content': evaluator_prompt},\n",
    "            {'role': 'user', 'content': f'''\n",
    "PROBLEM STATEMENT:\n",
    "{problem}\n",
    "\n",
    "GROUND TRUTH SOLUTION:\n",
    "{solution}\n",
    "\n",
    "SOLUTION PROPOSAL:\n",
    "{d['proposals'][best]}\n",
    "\n",
    "FINAL ANSWER #1:\n",
    "{ans1}\n",
    "\n",
    "FINAL ANSWER #2:\n",
    "{ans2}\n",
    "'''\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        outputs = llama.chat_completion(\n",
    "            [dialog for _ in range(3)],\n",
    "            max_gen_len=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            seed=42,\n",
    "        )\n",
    "        \n",
    "        baseline_vote = 0\n",
    "        cached_vote = 0\n",
    "        for evaluation in outputs:\n",
    "            match = re.search(r'VERDICT:\\s*(\\d)', evaluation['generation']['content'])\n",
    "            if match:\n",
    "                num = int(match.group(1))\n",
    "                if (num == 1 and baseline_first) or (num == 2 and not baseline_first):\n",
    "                    baseline_vote += 1\n",
    "                else:\n",
    "                    cached_vote += 1\n",
    "        \n",
    "        baseline_win += baseline_vote > cached_vote\n",
    "        cached_win += cached_vote > baseline_vote\n",
    "                \n",
    "    print(baseline_win, cached_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56310516-d7fa-4d1d-93f7-9fc461f53c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Converting to LoRA\n",
      "Loaded in 15.68 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from llama import Workflow\n",
    "from llama.workflows.tot import tot_cached, load_math_problems\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "workflow = Workflow.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=8192,\n",
    "    max_batch_size=4,\n",
    "    model_parallel_size=1,\n",
    "    max_nodes=20,\n",
    "    use_lora=True,\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(f'/scratch4/jeisner1/tjbai/checkpoints/lora_epoch-0_step-99.pt', weights_only=True)\n",
    "workflow.model.load_state_dict(checkpoint['lora'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddaaac89-97a9-48e3-9ffd-6a247d79249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = load_math_problems(\n",
    "    '../data/MATH',\n",
    "    split='train',\n",
    "    problem_types=['counting_and_probability']\n",
    ")[:200]\n",
    "\n",
    "from llama import Llama\n",
    "from llama.workflows.tot import benchmark_solution_quality\n",
    "\n",
    "llama = Llama(workflow.model, workflow.tokenizer)\n",
    "\n",
    "outputs = benchmark_solution_quality(\n",
    "    llama=llama,\n",
    "    workflow=workflow,\n",
    "    problem=problems[0]['problem'],\n",
    "    branching_factor=8,\n",
    "    voters=4,\n",
    "    compact=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbb9c4e-dda0-4c48-8bf4-4ebfded6e964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded in 14.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [00:59<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:00<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:00<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [00:53<00:00,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from llama import Llama\n",
    "from llama.workflows.tot import load_math_problems\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "llama = Llama.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=4096,\n",
    "    max_batch_size=3,\n",
    "    model_parallel_size=1,\n",
    ")\n",
    "\n",
    "evaluator_prompt = '''\n",
    "You are a strict evaluator for mathematics problems. You will assess:\n",
    "1. Problem statement \n",
    "2. Official solution and final answer\n",
    "3. Student's attempted solution and final answer\n",
    "\n",
    "Evaluation criteria:\n",
    "- Final answers must be mathematically equivalent to the official solution\n",
    "- All valid equivalent expressions are correct (e.g., 1/2 vs 0.5 vs 2^-1)\n",
    "\n",
    "Output: Respond with ONLY \"correct\" or \"incorrect\" based on the final answer.\n",
    "'''\n",
    "\n",
    "problems = load_math_problems('/home/tbai4/llama3/data/MATH', split='val')\n",
    "\n",
    "all = []\n",
    "\n",
    "for path in [\n",
    "    '/home/tbai4/llama3/dumps/tot_2/hotswap-epoch-0_step-195.json',\n",
    "    '/home/tbai4/llama3/dumps/tot_2/hotswap-epoch-0_step-395.json',\n",
    "    '/home/tbai4/llama3/dumps/tot_2/hotswap-epoch-0_step-595.json',\n",
    "    '/home/tbai4/llama3/dumps/tot_2/hotswap-epoch-0_step-795.json',\n",
    "    # '/home/tbai4/llama3/dumps/tot_2/lora_epoch-0_step-195.pt_e2e.json',\n",
    "    # '/home/tbai4/llama3/dumps/tot_2/lora_epoch-0_step-395.pt_e2e.json',\n",
    "    # '/home/tbai4/llama3/dumps/tot_2/lora_epoch-0_step-595.pt_e2e.json',\n",
    "    # '/home/tbai4/llama3/dumps/tot_2/lora_epoch-1_step-95.pt_e2e.json',\n",
    "    # '/home/tbai4/llama3/dumps/tot_2/lora_epoch-1_step-495.pt_e2e.json',\n",
    "    # '/home/tbai4/llama3/dumps/tot_2/lora_epoch-1_step-895.pt_e2e.json',\n",
    "    # '/home/tbai4/llama3/dumps/baseline_e2e.json',\n",
    "]:\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    correct = []\n",
    "    for d, problem_obj in tqdm(zip(data, problems), total=280):\n",
    "        problem = problem_obj['problem']\n",
    "        solution = problem_obj['solution']\n",
    "        attempt = llama.tokenizer.decode(d['final_tokens'])\n",
    "        \n",
    "        dialog = [\n",
    "            {'role': 'system', 'content': evaluator_prompt},\n",
    "            {'role': 'user', 'content': f'''\n",
    "PROBLEM STATEMENT:\n",
    "{problem}\n",
    "\n",
    "GROUND TRUTH SOLUTION:\n",
    "{solution}\n",
    "\n",
    "ATTEMPTED SOLUTION:\n",
    "{attempt}\n",
    "'''\n",
    "            }\n",
    "        ]\n",
    "            \n",
    "        outputs = llama.chat_completion(\n",
    "            [dialog for _ in range(3)],\n",
    "            max_gen_len=256,\n",
    "            temperature=0.25,\n",
    "            top_p=0.9,\n",
    "            seed=42,\n",
    "        )\n",
    "        \n",
    "        inc = 0\n",
    "        cor = 0\n",
    "        for o in outputs:\n",
    "            if 'incorrect' in o['generation']['content'].lower():\n",
    "                inc += 1\n",
    "            elif 'correct' in o['generation']['content'].lower():\n",
    "                cor += 1\n",
    "            else:\n",
    "                print(o['generation']['content'])\n",
    "                raise Exception()\n",
    "\n",
    "        correct.append(cor > inc)\n",
    "        \n",
    "    print(sum(correct))\n",
    "    all.append(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfc1f83-766f-4e95-95b3-8fd2de3e316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table:\n",
      "Both correct: 70\n",
      "Baseline correct only: 44\n",
      "Fine-tuned correct only: 36\n",
      "Both incorrect: 130\n",
      "\n",
      "p-value: 0.43404225534547874\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('tmp') as f:\n",
    "    all = json.load(f)\n",
    "    \n",
    "import numpy as np\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "def run_mcnemars_test(clf1_results, clf2_results):\n",
    "    clf1_results = np.array(clf1_results)\n",
    "    clf2_results = np.array(clf2_results)\n",
    "    \n",
    "    both_correct = np.sum((clf1_results == True) & (clf2_results == True))\n",
    "    both_incorrect = np.sum((clf1_results == False) & (clf2_results == False))\n",
    "    clf1_only = np.sum((clf1_results == True) & (clf2_results == False))\n",
    "    clf2_only = np.sum((clf1_results == False) & (clf2_results == True))\n",
    "    \n",
    "    table = [[both_correct, clf1_only],\n",
    "             [clf2_only, both_incorrect]]\n",
    "    \n",
    "    print(f\"Contingency Table:\")\n",
    "    print(f\"Both correct: {both_correct}\")\n",
    "    print(f\"Baseline correct only: {clf1_only}\")\n",
    "    print(f\"Fine-tuned correct only: {clf2_only}\")\n",
    "    print(f\"Both incorrect: {both_incorrect}\")\n",
    "    \n",
    "    # Run test\n",
    "    result = mcnemar(table, exact=True)  # exact=True for small samples\n",
    "    \n",
    "    return result.pvalue\n",
    "\n",
    "p_value = run_mcnemars_test(all[1], all[0])\n",
    "print(f\"\\np-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa345bf-0ca0-40fb-a759-0a12243a4030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
