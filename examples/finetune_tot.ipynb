{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37598c9a-64e3-4f91-8566-d0f76c1029c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%cd llama3/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99171e6f-6165-4597-89c2-20c906602ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Converting to LoRA\n",
      "Loaded in 18.88 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08480376642881861"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from llama import Workflow, Llama\n",
    "from llama.util import load_model_and_tokenizer\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "\n",
    "workflow = Workflow.build(\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    max_seq_len=8192,\n",
    "    max_batch_size=4,\n",
    "    model_parallel_size=1,\n",
    "    max_nodes=20,\n",
    "    use_lora=True,\n",
    "    lora_rank=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "workflow.model.get_trainable_param_percentage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e46ac587-8dc3-4a52-bceb-eb8f4226d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 6.8M / 8.0B parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama.workflows.finetune import TotTrainer\n",
    "from llama.workflows.tot import cot_prompt, finish_prompt, format_vote_system_prompt, format_problem\n",
    "\n",
    "sample = torch.load('tot_data/problem_0.pt', weights_only=True)\n",
    "trainer = TotTrainer(workflow, branching_factor=8, voters=4)\n",
    "\n",
    "cot, vote, finish = workflow.insert([\n",
    "    {'messages': [\n",
    "        {'role': 'system', 'content': cot_prompt},\n",
    "        {'role': 'user', 'content': format_problem(sample['problem'])}\n",
    "    ], 'parent_ids': []},\n",
    "    {'messages': [\n",
    "        {'role': 'system', 'content': format_vote_system_prompt(8)},\n",
    "        {'role': 'user', 'content': format_problem(sample['problem'])}\n",
    "    ], 'parent_ids': []},\n",
    "    {'messages': [\n",
    "        {'role': 'system', 'content': finish_prompt},\n",
    "        {'role': 'user', 'content': format_problem(sample['problem'])}\n",
    "    ], 'parent_ids': []},\n",
    "], training=True)\n",
    "\n",
    "proposal_tasks = [\n",
    "    {'header': ('assistant', None),\n",
    "     'prefill': f'Solution #{i+1}:\\n\\n',\n",
    "     'parent_ids': [cot['id']]}\n",
    "    for i in range(8)\n",
    "]\n",
    "target_proposal_ids = [p + [workflow.tokenizer.eot_id] for p in sample['result']['proposal_tokens']]\n",
    "proposal_nodes, proposal_logprobs = workflow.train_step(proposal_tasks, target_proposal_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de40b66f-721f-495c-a730-75130115a4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.reset()\n",
    "workflow.model.set_adapter_state(enabled=False)\n",
    "\n",
    "[system] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer the user\\'s question please.'}],\n",
    "        'parent_ids': [],\n",
    "    },\n",
    "])\n",
    "\n",
    "[user_1] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the capital of France?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "])\n",
    "\n",
    "[output], _ = workflow.step(\n",
    "    [\n",
    "        {\n",
    "            'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [system['id'], user_1['id']],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "workflow.tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24468385-fe66-42ba-a0bd-f9fd7cbb7418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.reset()\n",
    "workflow.model.set_adapter_state(enabled=True)\n",
    "\n",
    "[system] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer the user\\'s question please.'}],\n",
    "        'parent_ids': [],\n",
    "    },\n",
    "])\n",
    "\n",
    "[user_1] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the capital of France?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "])\n",
    "\n",
    "[output], _ = workflow.step(\n",
    "    [\n",
    "        {\n",
    "            'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [system['id'], user_1['id']],\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "workflow.tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6ac29-8c09-446b-b0ef-46d7803e1d5b",
   "metadata": {},
   "source": [
    "## sanity check forward-backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d67520-94c1-407f-a9ba-4ab77f476b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 6.8M / 8.0B parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama.workflows.finetune import TotTrainer\n",
    "from llama.workflows.tot import cot_prompt, finish_prompt, format_vote_system_prompt, format_problem\n",
    "\n",
    "problem = torch.load('tot_data/problem_0.pt', weights_only=True)\n",
    "trainer = TotTrainer(workflow, branching_factor=8, voters=4)\n",
    "\n",
    "workflow.model.train()\n",
    "workflow.model.set_adapter_state(enabled=True)\n",
    "workflow.model.zero_grad()\n",
    "\n",
    "total_loss, metrics = trainer.step(problem)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e555c4d9-c44c-4b75-9f53-366e5bc0c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.workflows.finetune import finetune\n",
    "\n",
    "finetune(\n",
    "    data_path='tot_data',\n",
    "    ckpt_dir='/scratch4/jeisner1/tjbai/llama_8b',\n",
    "    tokenizer_path='/scratch4/jeisner1/tjbai/llama_8b/tokenizer.model',\n",
    "    output_dir='/scratch4/jeisner1/tjbai/checkpoints',\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    max_seq_len=6144,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf32633-2499-4090-915f-0c3f3cc33d33",
   "metadata": {},
   "source": [
    "## load and run checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544edab-2825-4bc2-9e4b-7b2ddef5da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load('/scratch4/jeisner1/tjbai/checkpoints/lora_epoch-0_step-399.pt', weights_only=True)\n",
    "workflow.model.load_state_dict(checkpoint['lora'])\n",
    "\n",
    "workflow.reset()\n",
    "workflow.model.eval()\n",
    "workflow.model.set_adapter_state(True)\n",
    "\n",
    "[system] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'system', 'content': 'Answer ALL of the user\\'s question(s).'}],\n",
    "        'parent_ids': [],\n",
    "    },\n",
    "])\n",
    "\n",
    "[user_1, user_2] = workflow.insert([\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the capital of France?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "    {\n",
    "        'messages': [{'role': 'user', 'content': 'What is the largest planet in the solar system?'}],\n",
    "        'parent_ids': [system['id']],\n",
    "    },\n",
    "])\n",
    "\n",
    "[output], _ = workflow.step(\n",
    "    [\n",
    "        {\n",
    "            'header': ('assistant', None),\n",
    "            'prefill': '',\n",
    "            'parent_ids': [system['id'], user_1['id'], user_2['id']],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "workflow.tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a8cdf-36c3-4847-b078-656f02b9b904",
   "metadata": {},
   "source": [
    "## evaluate trick prompt results across checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942a2e8-9724-4e46-bb4b-e5ff3ba1df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from llama import Llama\n",
    "from llama.workflows.tot import load_math_problems, benchmark_tricky_tot\n",
    "from tqdm import tqdm\n",
    "\n",
    "problems = load_math_problems(\n",
    "    '../data/MATH',\n",
    "    split='train',\n",
    "    problem_types=['counting_and_probability']\n",
    ")[:100]\n",
    "\n",
    "for id in [99, 199, 299, 399]: \n",
    "    checkpoint = torch.load(f'/scratch4/jeisner1/tjbai/checkpoints/lora_epoch-0_step-{id}.pt', weights_only=True)\n",
    "    workflow.model.load_state_dict(checkpoint['lora'])\n",
    "    llama = Llama(workflow.model, workflow.tokenizer)\n",
    "    print(f'Loaded checkpoint-{id}')\n",
    "    print(f'Memory allocated: {torch.cuda.memory_allocated()}')\n",
    "\n",
    "    comps = []\n",
    "    for problem in tqdm(problems):\n",
    "        comps.append(benchmark_tricky_tot(\n",
    "            llama=llama,\n",
    "            workflow=workflow,\n",
    "            problem=problem['problem'],\n",
    "            branching_factor=8,\n",
    "            voters=4\n",
    "        ))\n",
    "        \n",
    "    with open(f'checkpoint-{id}.json', 'w') as f:\n",
    "        json.dump(comps, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382050c0-f3ef-430e-9fc8-ae265edad973",
   "metadata": {},
   "source": [
    "## generate and evaluate final solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a7f71-f8f9-4411-8606-9fdcf12e2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from llama import Llama\n",
    "from llama.workflows.tot import load_math_problems, benchmark_solution_quality\n",
    "from tqdm import tqdm\n",
    "\n",
    "problems = load_math_problems(\n",
    "    '../data/MATH',\n",
    "    split='train',\n",
    "    problem_types=['counting_and_probability']\n",
    ")[:200]\n",
    "\n",
    "for id in [99, 199, 299, 399]: \n",
    "    checkpoint = torch.load(f'/scratch4/jeisner1/tjbai/checkpoints/lora_epoch-0_step-{id}.pt', weights_only=True)\n",
    "    workflow.model.load_state_dict(checkpoint['lora'])\n",
    "    llama = Llama(workflow.model, workflow.tokenizer)\n",
    "    print(f'Loaded checkpoint-{id}')\n",
    "    print(f'Memory allocated: {torch.cuda.memory_allocated()}')\n",
    "\n",
    "    comps = []\n",
    "    for problem in tqdm(problems):\n",
    "        comps.append(benchmark_solution_quality(\n",
    "            llama=llama,\n",
    "            workflow=workflow,\n",
    "            problem=problem['problem'],\n",
    "            branching_factor=8,\n",
    "            voters=4,\n",
    "            compact=False,\n",
    "        ))\n",
    "        \n",
    "    with open(f'checkpoint-{id}_solution_quality.json', 'w') as f:\n",
    "        json.dump(comps, f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caef2ef-03ee-4845-a585-c49608ca4f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [00:25<06:12,  1.96s/it]"
     ]
    }
   ],
   "source": [
    "llama.model.set_adapter_state(enabled=False)\n",
    "\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from llama.workflows.tot import load_math_problems, benchmark_solution_quality, parse_choice\n",
    "\n",
    "\n",
    "evaluator_prompt = '''\n",
    "You are evaluating final answers to AMC/AIME competition problems. You will receive:\n",
    "\n",
    "1. A problem statement\n",
    "2. The ground truth solution \n",
    "3. A shared solution proposal that both contestants used\n",
    "4. Two final answers based on this proposal\n",
    "\n",
    "Your task is to evaluate how effectively each contestant converted the shared proposal into a valid solution.\n",
    "Note that valid solutions may differ from the ground truth approach while remaining correct.\n",
    "\n",
    "Evaluate both answers focusing on:\n",
    "1. Answer Format Quality\n",
    "- Clarity and conciseness of final statement\n",
    "- Proper mathematical notation\n",
    "- Inclusion of key numerical result\n",
    "\n",
    "2. Mathematical Validity\n",
    "- Correctness of final numerical answer\n",
    "- Completeness (all parts answered)\n",
    "- Any invalid mathematical claims\n",
    "\n",
    "3. Justification Level\n",
    "- Appropriate amount of supporting context\n",
    "- Balance between brevity and explanation\n",
    "- Clear connection to previous reasoning\n",
    "\n",
    "Walk through each of these criterion and compare the 2 solutions. \n",
    "\n",
    "You must format your response as:\n",
    "\n",
    "VERDICT: [1 or 2]\n",
    "VERDICT_NOTE: (one sentence explanation)\n",
    "'''\n",
    "\n",
    "problems = load_math_problems(\n",
    "    '../data/MATH',\n",
    "    split='train',\n",
    "    problem_types=['counting_and_probability']\n",
    ")[:200]\n",
    "\n",
    "for id in [0, 99, 199, 299, 399]:\n",
    "    with open(f'checkpoint-{id}_solution_quality.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    baseline_win = 0\n",
    "    cached_win = 0\n",
    "\n",
    "    for d, problem_obj in tqdm(zip(data, problems), total=200):\n",
    "        problem = d['problem']\n",
    "        solution = problem_obj['solution']\n",
    "        baseline_final = d['baseline_final']\n",
    "        cached_final = d['cached_final']\n",
    "\n",
    "        votes = [\n",
    "            choice for resp in d['voters'] if\n",
    "            (choice := parse_choice(resp)) is not None\n",
    "        ]\n",
    "        best = Counter(votes).most_common(1)[0][0] - 1\n",
    "\n",
    "        baseline_first = random.choice([True, False])\n",
    "        ans1 = baseline_final if baseline_first else cached_final \n",
    "        ans2 = cached_final if baseline_first else baseline_final\n",
    "\n",
    "        dialog = [\n",
    "            {'role': 'system', 'content': evaluator_prompt},\n",
    "            {'role': 'user', 'content': f'''\n",
    "PROBLEM STATEMENT:\n",
    "{problem}\n",
    "\n",
    "GROUND TRUTH SOLUTION:\n",
    "{solution}\n",
    "\n",
    "SOLUTION PROPOSAL:\n",
    "{d['proposals'][best]}\n",
    "\n",
    "FINAL ANSWER #1:\n",
    "{ans1}\n",
    "\n",
    "FINAL ANSWER #2:\n",
    "{ans2}\n",
    "'''\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        [evaluation] = llama.chat_completion(\n",
    "            [dialog],\n",
    "            max_gen_len=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        match = re.search(r'VERDICT:\\s*(\\d)', evaluation['generation']['content'])\n",
    "        if match:\n",
    "            num = int(match.group(1))\n",
    "            if (num == 1 and baseline_first) or (num == 2 and not baseline_first):\n",
    "                baseline_win += 1\n",
    "            else:\n",
    "                cached_win += 1\n",
    "                \n",
    "    print(baseline_win, cached_win)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
